"""
æç¤ºè¯
"""

import os

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage
from langchain_core.prompts import PromptTemplate

config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.env')
load_dotenv(config_path)

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = os.getenv('API_KEY')
os.environ["OPENAI_BASE_URL"] = os.getenv('BASE_URL')
model = os.getenv('MODEL')


def init_model(model):
    # åˆå§‹åŒ– LLM Model
    return init_chat_model(model=model,
                           model_provider="openai",  # æŒ‡å®šæ¨¡å‹å‚å•†
                           temperature=0.7,  # æ¸©åº¦ï¼Œæ§åˆ¶è¿”å›æ›´ç¨³å®šè¿˜æ˜¯æ›´æœ‰åˆ›é€ åŠ›çš„ç»“æœ
                           timeout=30,  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’
                           max_tokens=1000,  # é™åˆ¶å“åº”ä¸­çš„ä»¤ç‰Œæ€»æ•°ï¼Œä»è€Œæœ‰æ•ˆåœ°æ§åˆ¶è¾“å‡ºçš„é•¿åº¦ã€‚
                           max_retries=3,  # æœ€å¤§å¤±è´¥é‡è¯•æ¬¡æ•°
                           )


def pretty_print_ai_response(response):
    """
    ç¾åŒ–çš„ AI å“åº”è¾“å‡º
    :param response: å¤§æ¨¡å‹çš„è¿”å›
    :return:
    """
    separator = "=" * 60

    print(f"\n{separator}")
    print("ğŸ¤– AI æ™ºèƒ½å›å¤")
    print(separator)

    # ä¸»è¦å†…å®¹æ˜¾ç¤º
    print(f"\nğŸ’¬ å›å¤å†…å®¹:")
    if hasattr(response, 'content'):
        print(response.content)
    else:
        print(str(response))

        # æŠ€æœ¯ä¿¡æ¯
    print(f"\n{separator}")
    print("ğŸ“Š æŠ€æœ¯è¯¦æƒ…:")
    print(f"  ğŸ“ ç±»å‹: {type(response).__name__}")

    # Token ä½¿ç”¨æƒ…å†µ
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        print(f"  ğŸ’° Token: {response.usage_metadata}")
    elif hasattr(response, 'usage') and response.usage:
        print(f"  ğŸ’° Token: {response.usage}")
    else:
        print("  ğŸ’° Token: æœªæä¾›")

    print(separator)


def simple_text_prompt(model):
    """
    åŸºç¡€çš„åŸºäºModelçš„å¤§æ¨¡å‹åŒæ­¥è®¿é—®ï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ã€æ¸©åº¦ã€æœ€å¤§tokené™åˆ¶
    :param model:
    :return:
    """
    # åˆ›å»º LLM Model
    model = init_model(model)

    # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºæç¤ºè¯ï¼Œç›´æ¥åœ¨invokeä¸­ä¼ å…¥ä¸€ä¸ªå­—ç¬¦ä¸²
    res = model.invoke("å†™ä¸€é¦–å…³äºæ± å¡˜çš„å¤é£æ­Œæ›²")
    pretty_print_ai_response(res)

    # ä½¿ç”¨ Message å¯¹è±¡æ¥ä½œä¸ºä¼ é€æç¤ºè¯ï¼Œæ­¤æ—¶invokeæ¥æ”¶çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé€šå¸¸æ˜¯ SystemMessage -> HumanMessage -> AIMessage è¿™ç§é¡ºåºå¾ªç¯
    user_msg = HumanMessage("å†™ä¸€é¦–å…³äºå‹æƒ…çš„å¤é£æ­Œæ›²")
    res = model.invoke([user_msg])
    pretty_print_ai_response(res)

    # ä¹Ÿå¯ä»¥ä½¿ç”¨å­—å…¸æ ¼å¼ä¼ è¾“æç¤ºè¯
    msg_list = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¤é£ç¼–è¯ä½œæ›²å¤§å¸ˆï¼Œæ“…é•¿å†™å„ç§ç±»å‹çš„æ•…äº‹æ­Œæ›²"},
        {"role": "user", "content": "å†™ä¸€é¦–å…³äºäº‹ä¸šçš„å¤é£æ­Œæ›²"},
    ]
    res = model.invoke(msg_list)
    pretty_print_ai_response(res)


# simple_text_prompt("Qwen/Qwen3-8B")


def prompt_template(model):
    """
    æç¤ºè¯æ¨¡æ¿, åŸºäºpythonè¯­æ³•ä¸­çš„ f-string æ–¹å¼è¿›è¡Œå˜é‡æ›¿æ¢
    è¿™ç§æ–¹å¼ï¼Œæœ‰ä»¥ä¸‹é™åˆ¶:
    - ä¸æ”¯æŒåµŒå¥—è®¿é—®æ–¹å¼ï¼šå¦‚ {user.name} è¿™æ˜¯éæ³•çš„
    - ä¸æ”¯æŒæ ¼å¼åŒ–ï¼šå¦‚ {price:.2f} è¿™ç§ä¿ç•™ä¸¤ä½å°æ•°çš„æ–¹å¼ä¹Ÿä¸è¡Œ
    - ä¸æ”¯æŒè¡¨è¾¾å¼ï¼šå¦‚ {x+y} è¿™ç§ä¹Ÿéæ³•
    - ä¸æ”¯æŒå‡½æ•°è°ƒç”¨: å¦‚ {str.upper()} è¿™ç§ä¸è¡Œ
    - å¾ªç¯oræ¡ä»¶åˆ¤æ–­: ä¸æ”¯æŒ
    - æ•°ç»„é€‰æ‹©ï¼šä¸æ”¯æŒ {items[0]} è¿™ç§æ–¹å¼
    :param model:
    :return:
    """
    template = """
ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆï¼Œæ“…é•¿ç»“åˆå¤è¯—è¯ã€äº”è¡Œå…«å­—ç»™äººå–å‡ºå¥½å¬ã€å¯“æ„å¥½ã€äº”è¡Œåœ†æ»¡çš„åå­—ï¼Œä½ åº”è¯¥è¿”å›äº”ä¸ªåå­—ï¼Œå¹¶è§£é‡Šæ¯ä¸ªåå­—çš„å¯“æ„ã€‚
ä¸‹é¢æ˜¯éœ€è¦å–åçš„ä¿¡æ¯: 
{info}
"""
    prompt = PromptTemplate.format_prompt(template, info="26å¹´2æœˆ6æ—¥ 10:01åˆ†å‡ºç”Ÿçš„å°å¥³å­©ï¼Œå§“:é’±")
    # è½¬æ¢ä¸º HumanMessage
    # user_message = prompt.to_messages()
    # è½¬æ¢ä¸ºæ–‡æœ¬
    user_txt = prompt.to_string()
    print(user_txt)

    # res = init_model(model).invoke(user_txt)
    # pretty_print_ai_response(res)

    # å¦‚æœæç¤ºè¯ä¸­æœ¬èº«å°±æœ‰ {}ï¼Œæ¯”å¦‚æç¤ºè¯ä¸­æœ‰jsonçš„æ•°æ®ï¼Œæ­¤æ—¶é’ˆå¯¹ä¸éœ€è¦åšå…³é”®è¯æ›¿æ¢çš„åœ°æ–¹ï¼Œä½¿ç”¨åŒå±‚æ‹¬å· {{ }}
    print(PromptTemplate.format_prompt("ä¸éœ€è¦è½¬æ¢çš„ ={{not_var}}= éœ€è¦æ›¿æ¢çš„ ={info}=", info="å“ˆå“ˆâŒšï¸",
                                       not_var="ä¸è¢«æ›¿æ¢"))


def prompt_template_by_mustache(model = model):
    """
    æç¤ºè¯æ¨¡æ¿, åŸºäºmustacheè¯­æ³•è¿›è¡Œå˜é‡æ›¿æ¢
    :param model:
    :return:
    """
    # åŸºç¡€mustacheæ¨¡æ¿ç¤ºä¾‹
    template = """
ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆï¼Œæ“…é•¿ç»“åˆå¤è¯—è¯ã€äº”è¡Œå…«å­—ç»™äººå–å‡ºå¥½å¬ã€å¯“æ„å¥½ã€äº”è¡Œåœ†æ»¡çš„åå­—ï¼Œä½ åº”è¯¥è¿”å›äº”ä¸ªåå­—ï¼Œå¹¶è§£é‡Šæ¯ä¸ªåå­—çš„å¯“æ„ã€‚
ä¸‹é¢æ˜¯éœ€è¦å–åçš„ä¿¡æ¯: 
{{info}}
"""
    
    # æ­£ç¡®çš„mustacheæ¨¡æ¿åˆ›å»ºæ–¹å¼
    prompt_template = PromptTemplate(
        template=template,
        template_format="mustache",
        input_variables=["info"]
    )
    
    # æ­£ç¡®çš„æ ¼å¼åŒ–æ–¹å¼
    prompt = prompt_template.format(info="26å¹´2æœˆ6æ—¥ 10:01åˆ†å‡ºç”Ÿçš„å°å¥³å­©ï¼Œå§“:é’±")
    print("=== åŸºç¡€mustacheæ¨¡æ¿ ===")
    print(prompt)

    res = init_model(model).invoke(prompt)
    pretty_print_ai_response(res)
    
    # æ›´å¤æ‚çš„mustacheæ¨¡æ¿ç¤ºä¾‹ï¼Œç›´æ¥å‚è€ƒ MustacheTemplateDemo.py


# åªåœ¨éœ€è¦æ—¶è°ƒç”¨
prompt_template_by_mustache(model)
