import os

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.env')
load_dotenv(config_path)

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = os.getenv('API_KEY')
os.environ["OPENAI_BASE_URL"] = os.getenv('BASE_URL')
model = os.getenv('MODEL')


def init_model(model):
    # åˆå§‹åŒ– LLM Model
    return init_chat_model(model=model,
                           model_provider="openai",  # æŒ‡å®šæ¨¡å‹å‚å•†
                           temperature=0.7,  # æ¸©åº¦ï¼Œæ§åˆ¶è¿”å›æ›´ç¨³å®šè¿˜æ˜¯æ›´æœ‰åˆ›é€ åŠ›çš„ç»“æœ
                           timeout=30,  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’
                           max_tokens=1000,  # é™åˆ¶å“åº”ä¸­çš„ä»¤ç‰Œæ€»æ•°ï¼Œä»è€Œæœ‰æ•ˆåœ°æ§åˆ¶è¾“å‡ºçš„é•¿åº¦ã€‚
                           max_retries=3,  # æœ€å¤§å¤±è´¥é‡è¯•æ¬¡æ•°
                           )


def pretty_print_ai_response(response):
    """
    ç¾åŒ–çš„ AI å“åº”è¾“å‡º
    :param response: å¤§æ¨¡å‹çš„è¿”å›
    :return:
    """
    separator = "=" * 60

    print(f"\n{separator}")
    print("ğŸ¤– AI æ™ºèƒ½å›å¤")
    print(separator)

    # ä¸»è¦å†…å®¹æ˜¾ç¤º
    print(f"\nğŸ’¬ å›å¤å†…å®¹:")
    if hasattr(response, 'content'):
        print(response.content)
    else:
        print(str(response))

        # æŠ€æœ¯ä¿¡æ¯
    print(f"\n{separator}")
    print("ğŸ“Š æŠ€æœ¯è¯¦æƒ…:")
    print(f"  ğŸ“ ç±»å‹: {type(response).__name__}")

    # Token ä½¿ç”¨æƒ…å†µ
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        print(f"  ğŸ’° Token: {response.usage_metadata}")
    elif hasattr(response, 'usage') and response.usage:
        print(f"  ğŸ’° Token: {response.usage}")
    else:
        print("  ğŸ’° Token: æœªæä¾›")

    print(separator)


def basic_call(model):
    """
    åŸºç¡€æ¶ˆæ¯ä½¿ç”¨ç¤ºä¾‹
    å±•ç¤ºSystemMessageã€HumanMessageã€AIMessageçš„åŸºæœ¬ç”¨æ³•
    """
    model = init_model(model)

    # ç³»ç»Ÿæç¤ºè¯ - è®¾å®šAIè§’è‰²å’Œè¡Œä¸º
    system_msg = SystemMessage("ä½ æ˜¯ä¸€ä¸ªå¹½é»˜çš„èŠå¤©å¤§å¸ˆï¼Œæ“…é•¿ä»¥å„ç§é£è¶£ã€æœ‰æ¢—çš„è¯è¯­å’Œäººäº¤æµæ²Ÿé€š")

    # ç”¨æˆ·è¾“å…¥ - æ¨¡æ‹ŸçœŸå®å¯¹è¯
    human_msg = HumanMessage("ä»Šå¤©ä¸Šç­çš„è·¯ä¸Šï¼Œçœ‹åˆ°ä¸€æ¡ç‹—åœ¨è¿½æ±½è½¦")

    # AIå†å²å›å¤ - å±•ç¤ºå¯¹è¯å»¶ç»­æ€§
    ai_msg = AIMessage(
        "å•Šå‘€ï¼Œè¿™ä¸å°±æ˜¯ä¼ è¯´ä¸­çš„\"ç‹—è¿½è±ªåç‰ˆæ±½è½¦\"å—ï¼Ÿæƒ³æƒ³çœ‹ï¼Œè¦æ˜¯æœ‰äººåœ¨å…¬å¸å¹´ä¼šä¸ŠæŠ½ä¸€è¾†è½¦ï¼Œä¼°è®¡é‚£æ¡ç‹—éƒ½ä¼šæ›¿ä¸»äººå¼€å¿ƒå¾—æ‘‡å°¾å·´å§ï¼")

    # ç»§ç»­å¯¹è¯
    res = model.invoke([system_msg, human_msg, ai_msg, HumanMessage("ä¸€èˆ¬çš„å…¬å¸å¹´ä¼šå¯ä¸ä¼šæœ‰è½¦ä½œä¸ºå¥–å“äº†~")])
    pretty_print_ai_response(res)


if __name__ == "__main__":
    basic_call(model)
    # æ›´å¤šçš„ç¤ºä¾‹ï¼Œå¯ä»¥å‚ç…§ MessageComprehensiveDemo.py
