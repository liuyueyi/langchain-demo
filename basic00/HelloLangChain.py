# åŠ è½½ç¯å¢ƒå˜é‡
import os

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.env')
load_dotenv(config_path)

llm = ChatOpenAI(api_key=os.getenv('API_KEY'),
                 base_url=os.getenv('BASE_URL'),
                 model=os.getenv('MODEL'),
                 stream_usage=True)

messages = [
    (
        "system",
        "ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„å¹½é»˜å¤§å¸ˆï¼Œæ“…é•¿å„ç§ç½‘ç»œçƒ­æ¢—å’Œå¹½é»˜æ®µå­ï¼Œæ€»æ˜¯ä»¥é£è¶£çš„å£å»ä¸ä»–äººå¯¹è¯",
    ),
    ("human", "æˆ‘æƒ³ä¸Šè¡—ä¹°ä¸ªå¸½å­"),
]

ai_msg = llm.invoke(messages)

# ç¾åŒ–è¾“å‡ºç»“æœ
print("\n" + "=" * 60)
print("ğŸ¤– AI æ™ºèƒ½å›å¤")
print("=" * 60)
print(f"\nğŸ’¬ å›å¤å†…å®¹:\n{ai_msg.content}")
print("\n" + "=" * 60)
print("ğŸ“Š è¯¦ç»†æŠ€æœ¯ä¿¡æ¯:")
print(f"  ğŸ“ æ¶ˆæ¯ç±»å‹: {type(ai_msg).__name__}")
if hasattr(ai_msg, 'usage_metadata') and ai_msg.usage_metadata:
    print(f"  ğŸ’° Token ä½¿ç”¨: {ai_msg.usage_metadata}")
else:
    print("  ğŸ’° Token ä½¿ç”¨: æœªæä¾›")
print(f"  ğŸ” å¯¹è±¡å±æ€§æ•°: {len([attr for attr in dir(ai_msg) if not attr.startswith('_')])} ä¸ª")
print("=" * 60)

# ---------------------------------------------- åˆ†å‰²


# æµå¼è¿”å›å¤„ç†
print("\n" + "=" * 60)
print("ğŸ¤– AI æµå¼å›å¤ä¸­...")
print("=" * 60)

# æ”¶é›†å®Œæ•´çš„å›å¤å†…å®¹
full_response = ""

# ä½¿ç”¨æµå¼æ–¹æ³•é€ä¸ªå¤„ç†è¿”å›çš„token
for chunk in llm.stream(messages):
    # æ‰“å°æ¯ä¸ªchunkçš„å†…å®¹ï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰
    if hasattr(chunk, 'content') and chunk.content:
        print(chunk.content, end='', flush=True)
        full_response += chunk.content

# ç¾åŒ–è¾“å‡ºå®Œæ•´ç»“æœ
print("\n" + "=" * 60)
print("ğŸ“Š è¯¦ç»†æŠ€æœ¯ä¿¡æ¯:")
print(f"  ğŸ“ æ¶ˆæ¯ç±»å‹: {type(chunk).__name__}")
if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
    print(f"  ğŸ’° Token ä½¿ç”¨: {chunk.usage_metadata}")
else:
    print("  ğŸ’° Token ä½¿ç”¨: æœªæä¾›")
print(f"  ğŸ” å¯¹è±¡å±æ€§æ•°: {len([attr for attr in dir(chunk) if not attr.startswith('_')])} ä¸ª")
print("=" * 60)
