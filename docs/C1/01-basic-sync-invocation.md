# LangChainå®æˆ˜å¼€å‘æ•™ç¨‹ï¼ˆä¸€ï¼‰ï¼šåŸºç¡€åŒæ­¥è°ƒç”¨è¯¦è§£

> **æŒæ¡LangChainæ ¸å¿ƒæŠ€èƒ½**ï¼šä»åŸºç¡€åŒæ­¥è°ƒç”¨å¼€å§‹ä½ çš„AIåº”ç”¨å¼€å‘ä¹‹æ—…

## ğŸ¯ æœ¬æ–‡ç›®æ ‡

æ·±å…¥ç†è§£LangChainåŸºç¡€åŒæ­¥è°ƒç”¨çš„æ ¸å¿ƒæœºåˆ¶ï¼ŒæŒæ¡ç¯å¢ƒé…ç½®ã€æ¨¡å‹åˆå§‹åŒ–ã€å‚æ•°è°ƒä¼˜ç­‰å…³é”®æŠ€æœ¯è¦ç‚¹ã€‚

## ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹æ¦‚è§ˆ

é€šè¿‡æœ¬æ–‡ä½ å°†æŒæ¡ï¼š
- **ç¯å¢ƒé…ç½®ç®¡ç†**ï¼šå®‰å…¨çš„APIå¯†é’¥ç®¡ç†æ–¹å¼
- **æ¨¡å‹åˆå§‹åŒ–å‚æ•°**ï¼šæ¸©åº¦ã€è¶…æ—¶ã€é‡è¯•ç­‰å…³é”®é…ç½®
- **åŒæ­¥è°ƒç”¨å®ç°**ï¼šåŸºç¡€çš„invokeæ–¹æ³•ä½¿ç”¨
- **è¾“å‡ºç¾åŒ–æŠ€å·§**ï¼šä¸“ä¸šçš„å“åº”ç»“æœæ˜¾ç¤º
- **è°ƒè¯•ä¿¡æ¯åˆ†æ**ï¼šTokenä½¿ç”¨å’Œæ€§èƒ½ç›‘æ§

## ğŸ”§ åŸºç¡€åŒæ­¥è°ƒç”¨æ ¸å¿ƒæŠ€æœ¯

### ä»€ä¹ˆæ˜¯åŒæ­¥è°ƒç”¨ï¼Ÿ

åŒæ­¥è°ƒç”¨æ˜¯æŒ‡ç¨‹åºå‘é€è¯·æ±‚åç­‰å¾…æ¨¡å‹å®Œå…¨å¤„ç†å®Œæ¯•å†è¿”å›ç»“æœçš„æ–¹å¼ã€‚è¿™æ˜¯æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€é‡è¦çš„AIè°ƒç”¨æ¨¡å¼ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

âœ… **ç®€å•ç›´è§‚**ï¼šä»£ç é€»è¾‘æ¸…æ™°æ˜“æ‡‚  
âœ… **å®Œæ•´ç»“æœ**ï¼šä¸€æ¬¡æ€§è·å–å…¨éƒ¨å“åº”å†…å®¹  
âœ… **æ˜“äºè°ƒè¯•**ï¼šä¾¿äºåˆ†æå’Œæµ‹è¯•  
âœ… **ç¨³å®šå¯é **ï¼šæˆç†Ÿçš„è°ƒç”¨æ¨¡å¼

### é€‚ç”¨åœºæ™¯

- ç®€å•çš„é—®ç­”äº¤äº’
- å†…å®¹ç”Ÿæˆä»»åŠ¡
- æ•°æ®å¤„ç†å’Œåˆ†æ
- åŸå‹å¼€å‘å’Œæµ‹è¯•

## ğŸš€ æ ¸å¿ƒå®ç°è§£æ

### 1. ç¯å¢ƒé…ç½®åŠ è½½

```python
import os
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model

# æ™ºèƒ½è·¯å¾„æŸ¥æ‰¾é…ç½®æ–‡ä»¶
config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.env')
load_dotenv(config_path)

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡æ˜ å°„
os.environ["OPENAI_API_KEY"] = os.getenv('API_KEY')
os.environ["OPENAI_BASE_URL"] = os.getenv('BASE_URL')
model_name = os.getenv('MODEL')
```

**å…³é”®è¦ç‚¹**ï¼š
- ä½¿ç”¨ç›¸å¯¹è·¯å¾„ç¡®ä¿é…ç½®æ–‡ä»¶å¯æ‰¾åˆ°
- é€šè¿‡ç¯å¢ƒå˜é‡æ˜ å°„å®ç°é…ç½®æ ‡å‡†åŒ–
- æ”¯æŒå¤šç¯å¢ƒé…ç½®ç®¡ç†

### 2. æ¨¡å‹åˆå§‹åŒ–é…ç½®

```python
from langchain.chat_models import init_chat_model

def init_model(model):
    """åˆå§‹åŒ–LLMæ¨¡å‹é…ç½®"""
    return init_chat_model(
        model=model,
        model_provider="openai",      # æŒ‡å®šæ¨¡å‹æä¾›å•†
        temperature=0.7,              # æ¸©åº¦å‚æ•°ï¼š0-1ï¼Œæ§åˆ¶åˆ›é€ æ€§
        timeout=30,                   # è¶…æ—¶æ—¶é—´ï¼š30ç§’
        max_tokens=1000,              # æœ€å¤§è¾“å‡ºtokenæ•°
        max_retries=3                 # æœ€å¤§é‡è¯•æ¬¡æ•°
    )
```

**å‚æ•°è¯¦è§£**ï¼š

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ | å½±å“ |
|------|------|--------|------|
| temperature | è¾“å‡ºéšæœºæ€§ | 0.7 | è¶Šé«˜è¶Šcreative |
| timeout | ç­‰å¾…è¶…æ—¶ | 30ç§’ | é˜²æ­¢é•¿æ—¶é—´é˜»å¡ |
| max_tokens | è¾“å‡ºé•¿åº¦é™åˆ¶ | 1000 | æ§åˆ¶æˆæœ¬å’Œå“åº”æ—¶é—´ |
| max_retries | å¤±è´¥é‡è¯• | 3æ¬¡ | æé«˜è°ƒç”¨æˆåŠŸç‡ |


è¿™ç§ä¸»è¦æ˜¯å€ŸåŠ©langchainçš„ `init_chat_model` æ¥å®ç°modelåˆå§‹åŒ–ï¼Œæ¯”å¦‚æˆ‘ä»¬è¿™é‡Œè°ç”¨OpenAIçš„æ¥å£é£æ ¼ï¼Œå°†`API_KEY/BASE_URL`ç»´æŠ¤åœ¨ä¸Šé¢çš„ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­ï¼Œå¦‚æœæˆ‘çš„é¡¹ç›®ä¸­ï¼Œæœ‰å¤šä¸ªOpenAIé£æ ¼çš„æ¨¡å‹éœ€è¦å‘¢ï¼Œæ˜¾ç„¶è¿™ç§æ–¹å¼å°±ä¸å¤ªåˆé€‚äº†

æ­¤æ—¶æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨ `init_chat_model` ä¸­ä¼ å…¥å‚æ•°ï¼Œæ¥è¾¾åˆ°ä¸åŒåœºæ™¯çš„å‚æ•°é…ç½®ã€‚

```python
from langchain.chat_models import init_chat_model

model = init_chat_model(
    model="MODEL_NAME",
    model_provider="openai",
    base_url="BASE_URL",
    api_key="YOUR_API_KEY",
)
```

å½“ç„¶é™¤äº†ä¸Šé¢è¿™ç§æ–¹å¼ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ `langchain_openai` å®ç°çš„ChatOpenAIæ¥åˆ›å»ºmodelï¼Œæ¯”å¦‚ä¸‹é¢è¿™ä¸ªé€šè¿‡ä»£ç†æ¥ç›´æ¥è®¿é—®ChatGptçš„å¤§æ¨¡å‹ï¼Œé€šè¿‡ `openai_proxy` æ¥è®¾ç½®ä»£ç†ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡api_key, base_urlæ¥æŒ‡å®šæ”¯æŒOpenAIæ¥å£é£æ ¼çš„å¤§æ¨¡å‹ï¼‰

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4.1",
    openai_proxy="http://proxy.example.com:8080"
)
```

### 3. åŒæ­¥è°ƒç”¨æ ¸å¿ƒå®ç°

```python
def simple_invoke(model):
    """åŸºç¡€çš„åŒæ­¥è°ƒç”¨å®ç°"""
    # åˆå§‹åŒ–æ¨¡å‹å®ä¾‹
    model_instance = init_model(model)
    
    # æ‰§è¡ŒåŒæ­¥è°ƒç”¨
    response = model_instance.invoke("è¯·å†™ä¸€é¦–å…³äºé¢œè‰²çš„äº”è¨€ç»å¥")
    
    # ç¾åŒ–è¾“å‡ºç»“æœ
    pretty_print_ai_response(response)
```

## ğŸ’¡ å…³é”®æŠ€æœ¯è¦ç‚¹

### 1. å“åº”ç¾åŒ–è¾“å‡º

```python
def pretty_print_ai_response(response):
    """ä¸“ä¸šçš„AIå“åº”ç¾åŒ–è¾“å‡º"""
    separator = "=" * 60
    
    print(f"\n{separator}")
    print("ğŸ¤– AI æ™ºèƒ½å›å¤")
    print(separator)
    
    # ä¸»è¦å†…å®¹æ˜¾ç¤º
    print(f"\nğŸ’¬ å›å¤å†…å®¹:")
    if hasattr(response, 'content'):
        print(response.content)
    else:
        print(str(response))
    
    # æŠ€æœ¯ä¿¡æ¯åˆ†æ
    print(f"\n{separator}")
    print("ğŸ“Š æŠ€æœ¯è¯¦æƒ…:")
    print(f"  ğŸ“ ç±»å‹: {type(response).__name__}")
    
    # Tokenä½¿ç”¨æƒ…å†µ
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        print(f"  ğŸ’° Token: {response.usage_metadata}")
    elif hasattr(response, 'usage') and response.usage:
        print(f"  ğŸ’° Token: {response.usage}")
    else:
        print("  ğŸ’° Token: æœªæä¾›")
    
    # å¯¹è±¡å±æ€§ç»Ÿè®¡
    attr_count = len([attr for attr in dir(response) if not attr.startswith('_')])
    print(f"  ğŸ” å±æ€§æ•°: {attr_count} ä¸ª")
    print(separator)
```

### 2. å‚æ•°è°ƒä¼˜ç­–ç•¥

```python
def optimized_invoke(model, prompt, **kwargs):
    """å‚æ•°ä¼˜åŒ–çš„è°ƒç”¨å‡½æ•°"""
    # é»˜è®¤å‚æ•°é…ç½®
    default_params = {
        'temperature': 0.7,
        'max_tokens': 1000,
        'timeout': 30
    }
    
    # åˆå¹¶ç”¨æˆ·å‚æ•°
    params = {**default_params, **kwargs}
    
    # åŠ¨æ€åˆå§‹åŒ–æ¨¡å‹
    model_instance = init_chat_model(
        model=model,
        model_provider="openai",
        **params
    )
    
    return model_instance.invoke(prompt)

# ä½¿ç”¨ç¤ºä¾‹
def demo_parameter_tuning():
    """å‚æ•°è°ƒä¼˜æ¼”ç¤º"""
    prompts = ["å†™ä¸€é¦–è¯—", "è¯¦ç»†è§£é‡Šé‡å­åŠ›å­¦", "ç”ŸæˆæŠ€æœ¯æ–‡æ¡£"]
    
    # ä¸åŒåœºæ™¯çš„å‚æ•°é…ç½®
    configs = [
        {'temperature': 0.9, 'max_tokens': 200},    # åˆ›æ„å†™ä½œ
        {'temperature': 0.3, 'max_tokens': 1500},   # æŠ€æœ¯è§£é‡Š  
        {'temperature': 0.5, 'max_tokens': 800}     # æ–‡æ¡£ç”Ÿæˆ
    ]
    
    for prompt, config in zip(prompts, configs):
        print(f"\nğŸ“ åœºæ™¯: {prompt}")
        print(f"âš™ï¸  é…ç½®: {config}")
        response = optimized_invoke(model_name, prompt, **config)
        print(f"âœ… ç»“æœé•¿åº¦: {len(response.content)} å­—ç¬¦")
```

## ğŸ¯ å®æˆ˜åº”ç”¨åœºæ™¯

### åœºæ™¯1ï¼šå†…å®¹åˆ›ä½œåŠ©æ‰‹

```python
def content_creation_assistant():
    """å†…å®¹åˆ›ä½œåŠ©æ‰‹å®ç°"""
    
    class ContentCreator:
        def __init__(self, model):
            self.model = init_model(model)
        
        def write_poem(self, theme, style="å¤å…¸"):
            """å†™è¯—åŠŸèƒ½"""
            prompt = f"è¯·ç”¨{style}é£æ ¼å†™ä¸€é¦–å…³äº{theme}çš„è¯—"
            return self.model.invoke(prompt).content
        
        def generate_article(self, topic, length="short"):
            """ç”Ÿæˆæ–‡ç« """
            length_map = {"short": 300, "medium": 800, "long": 1500}
            prompt = f"å†™ä¸€ç¯‡å…³äº{topic}çš„æ–‡ç« ï¼Œçº¦{length_map[length]}å­—"
            return self.model.invoke(prompt).content
        
        def create_story(self, genre, characters=None):
            """åˆ›ä½œæ•…äº‹"""
            char_desc = f"ä¸»è§’æ˜¯{characters}" if characters else ""
            prompt = f"åˆ›ä½œä¸€ä¸ª{genre}æ•…äº‹å¼€å¤´{char_desc}"
            return self.model.invoke(prompt).content
    
    # ä½¿ç”¨ç¤ºä¾‹
    creator = ContentCreator(model_name)
    
    poem = creator.write_poem("æ˜¥å¤©")
    print("ğŸŒ¸ æ˜¥å¤©è¯—æ­Œ:")
    print(poem)
    
    article = creator.generate_article("äººå·¥æ™ºèƒ½å‘å±•", "short")
    print("\nğŸ“° AIå‘å±•çŸ­æ–‡:")
    print(article[:200] + "...")
```

### åœºæ™¯2ï¼šä»£ç ç”Ÿæˆå·¥å…·

```python
def code_generation_tool():
    """ä»£ç ç”Ÿæˆå·¥å…·å®ç°"""
    
    class CodeGenerator:
        def __init__(self, model):
            self.model = init_model(model)
        
        def generate_function(self, purpose, language="Python"):
            """ç”Ÿæˆå‡½æ•°ä»£ç """
            prompt = f"""ç”¨{language}å†™ä¸€ä¸ª{purpose}çš„å‡½æ•°
è¦æ±‚ï¼š
1. åŒ…å«è¯¦ç»†æ³¨é‡Š
2. éµå¾ªPEP8è§„èŒƒ
3. æ·»åŠ é”™è¯¯å¤„ç†"""
            
            response = self.model.invoke(prompt)
            return self.extract_code_block(response.content)
        
        def explain_code(self, code):
            """è§£é‡Šä»£ç åŠŸèƒ½"""
            prompt = f"è¯·è§£é‡Šä»¥ä¸‹ä»£ç çš„åŠŸèƒ½å’Œå®ç°æ€è·¯ï¼š\n\n{code}"
            return self.model.invoke(prompt).content
        
        def extract_code_block(self, text):
            """æå–ä»£ç å—"""
            import re
            code_blocks = re.findall(r'```(?:\w+)?\n(.*?)```', text, re.DOTALL)
            return code_blocks[0] if code_blocks else text
    
    # ä½¿ç”¨ç¤ºä¾‹
    coder = CodeGenerator(model_name)
    
    # ç”Ÿæˆæ’åºå‡½æ•°
    sort_function = coder.generate_function("å†’æ³¡æ’åºç®—æ³•")
    print("ğŸ ç”Ÿæˆçš„æ’åºå‡½æ•°:")
    print(sort_function)
    
    # è§£é‡Šä»£ç 
    explanation = coder.explain_code(sort_function)
    print("\nğŸ“ ä»£ç è§£é‡Š:")
    print(explanation[:300] + "...")
```

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. è°ƒç”¨æ€§èƒ½ç›‘æ§

```python
import time

def performance_monitoring_invoke(model, prompt):
    """å¸¦æ€§èƒ½ç›‘æ§çš„è°ƒç”¨å‡½æ•°"""
    start_time = time.time()
    
    try:
        model_instance = init_model(model)
        response = model_instance.invoke(prompt)
        end_time = time.time()
        
        # æ€§èƒ½æŒ‡æ ‡è®¡ç®—
        execution_time = end_time - start_time
        content_length = len(response.content)
        tokens_used = getattr(response, 'usage_metadata', {}).get('total_tokens', 0)
        
        performance_data = {
            'execution_time': execution_time,
            'content_length': content_length,
            'tokens_used': tokens_used,
            'efficiency': content_length / execution_time if execution_time > 0 else 0,
            'cost_efficiency': tokens_used / content_length if content_length > 0 else 0
        }
        
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {content_length}å­—ç¬¦")
        print(f"ğŸ’° Tokenæ¶ˆè€—: {tokens_used}")
        print(f"âš¡ æ•ˆç‡æŒ‡æ ‡: {performance_data['efficiency']:.1f}å­—ç¬¦/ç§’")
        
        return response, performance_data
        
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        return None, {'error': str(e)}

# æ€§èƒ½æµ‹è¯•
def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    test_prompts = [
        "å†™ä¸€å¥è¯",
        "å†™ä¸€æ®µè¯", 
        "å†™ä¸€ç¯‡æ–‡ç« "
    ]
    
    results = []
    for prompt in test_prompts:
        print(f"\nğŸ¯ æµ‹è¯•: {prompt}")
        response, metrics = performance_monitoring_invoke(model_name, prompt)
        if response:
            results.append(metrics)
    
    # ç»Ÿè®¡åˆ†æ
    if results:
        avg_time = sum(r['execution_time'] for r in results) / len(results)
        avg_efficiency = sum(r['efficiency'] for r in results) / len(results)
        print(f"\nğŸ“ˆ å¹³å‡æ€§èƒ½:")
        print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.2f}ç§’")
        print(f"  å¹³å‡æ•ˆç‡: {avg_efficiency:.1f}å­—ç¬¦/ç§’")
```

### 2. é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

```python
import random

def robust_invoke(model, prompt, max_retries=3, base_delay=1):
    """å…·æœ‰é‡è¯•æœºåˆ¶çš„ç¨³å¥è°ƒç”¨"""
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ“¡ å°è¯•ç¬¬ {attempt + 1} æ¬¡è°ƒç”¨...")
            
            model_instance = init_model(model)
            response = model_instance.invoke(prompt)
            
            print("âœ… è°ƒç”¨æˆåŠŸ!")
            return response
            
        except Exception as e:
            last_exception = e
            print(f"âŒ ç¬¬ {attempt + 1} æ¬¡è°ƒç”¨å¤±è´¥: {e}")
            
            if attempt < max_retries - 1:
                # æŒ‡æ•°é€€é¿å»¶è¿Ÿ
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                print(f"â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                time.sleep(delay)
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    print(f"ğŸ’¥ æ‰€æœ‰ {max_retries} æ¬¡é‡è¯•éƒ½å¤±è´¥")
    raise last_exception

# ä½¿ç”¨ç¤ºä¾‹
def demo_robust_calling():
    """ç¨³å¥è°ƒç”¨æ¼”ç¤º"""
    try:
        response = robust_invoke(
            model_name, 
            "è¯·å†™ä¸€é¦–å…³äºåšéŸ§ä¸æ‹”ç²¾ç¥çš„è¯—",
            max_retries=3
        )
        print("ğŸ“¬ æœ€ç»ˆç»“æœ:")
        print(response.content)
    except Exception as e:
        print(f"æœ€ç»ˆå¤±è´¥: {e}")
```

## ğŸ›¡ï¸ å®‰å…¨ä¸æœ€ä½³å®è·µ

### 1. é…ç½®å®‰å…¨ç®¡ç†

```python
class SecureConfigManager:
    def __init__(self, config_path):
        self.config_path = config_path
        self.required_keys = ['API_KEY', 'BASE_URL', 'MODEL']
        self.optional_keys = ['DEBUG', 'TIMEOUT']
    
    def validate_config(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        load_dotenv(self.config_path)
        
        missing_keys = []
        for key in self.required_keys:
            if not os.getenv(key):
                missing_keys.append(key)
        
        if missing_keys:
            raise ValueError(f"ç¼ºå°‘å¿…è¦é…ç½®é¡¹: {missing_keys}")
        
        print("âœ… é…ç½®éªŒè¯é€šè¿‡")
        return True
    
    def get_secure_config(self):
        """è·å–å®‰å…¨çš„é…ç½®ä¿¡æ¯"""
        self.validate_config()
        
        return {
            'api_key': os.getenv('API_KEY')[:8] + '...' if os.getenv('API_KEY') else None,
            'base_url': os.getenv('BASE_URL'),
            'model': os.getenv('MODEL'),
            'debug': os.getenv('DEBUG', 'false').lower() == 'true'
        }

# ä½¿ç”¨ç¤ºä¾‹
def secure_setup_demo():
    """å®‰å…¨é…ç½®æ¼”ç¤º"""
    config_manager = SecureConfigManager('config.env')
    
    try:
        secure_config = config_manager.get_secure_config()
        print("ğŸ” å®‰å…¨é…ç½®ä¿¡æ¯:")
        for key, value in secure_config.items():
            print(f"  {key}: {value}")
    except ValueError as e:
        print(f"é…ç½®é”™è¯¯: {e}")
```

### 2. æ—¥å¿—è®°å½•ä¸ç›‘æ§

```python
import logging
from datetime import datetime

class InvocationLogger:
    def __init__(self, log_file="invocation.log"):
        self.logger = logging.getLogger('LangChainInvoke')
        self.logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
    
    def log_invocation(self, prompt, response=None, error=None):
        """è®°å½•è°ƒç”¨æ—¥å¿—"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'prompt_length': len(prompt),
            'prompt_preview': prompt[:50] + '...' if len(prompt) > 50 else prompt
        }
        
        if response:
            log_data.update({
                'success': True,
                'response_length': len(response.content) if hasattr(response, 'content') else 0,
                'token_usage': getattr(response, 'usage_metadata', {}).get('total_tokens', 0)
            })
            self.logger.info(f"è°ƒç”¨æˆåŠŸ: {log_data}")
        else:
            log_data.update({
                'success': False,
                'error': str(error)
            })
            self.logger.error(f"è°ƒç”¨å¤±è´¥: {log_data}")

# ä½¿ç”¨ç¤ºä¾‹
def logged_invoke(model, prompt):
    """å¸¦æ—¥å¿—è®°å½•çš„è°ƒç”¨"""
    logger = InvocationLogger()
    
    try:
        model_instance = init_model(model)
        response = model_instance.invoke(prompt)
        logger.log_invocation(prompt, response)
        return response
    except Exception as e:
        logger.log_invocation(prompt, error=e)
        raise

# æµ‹è¯•æ—¥å¿—åŠŸèƒ½
def logging_demo():
    """æ—¥å¿—è®°å½•æ¼”ç¤º"""
    response = logged_invoke(model_name, "å†™ä¸€å¥æ¿€åŠ±äººå¿ƒçš„è¯")
    print("ğŸ’¬ ç”Ÿæˆå†…å®¹:")
    print(response.content)
    print("\nğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ° invocation.log æ–‡ä»¶")
```

## ğŸ“Š è°ƒç”¨æ¨¡å¼å¯¹æ¯”åˆ†æ

### åŒæ­¥ vs æµå¼ vs æ‰¹é‡

| ç‰¹æ€§ | åŒæ­¥è°ƒç”¨ | æµå¼è°ƒç”¨ | æ‰¹é‡è°ƒç”¨ |
|------|----------|----------|----------|
| å®ç°å¤æ‚åº¦ | ç®€å• | ä¸­ç­‰ | å¤æ‚ |
| ç”¨æˆ·ä½“éªŒ | ç­‰å¾…å®Œæ•´ç»“æœ | å®æ—¶é€å­—æ˜¾ç¤º | å¹¶è¡Œå¤„ç† |
| èµ„æºåˆ©ç”¨ | ä¸€èˆ¬ | è¾ƒå¥½ | æœ€ä¼˜ |
| é€‚ç”¨åœºæ™¯ | ç®€å•äº¤äº’ | é•¿æ–‡æœ¬ç”Ÿæˆ | å¤šä»»åŠ¡å¤„ç† |
| è°ƒè¯•å‹å¥½æ€§ | æœ€å¥½ | ä¸­ç­‰ | å¤æ‚ |

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
def comparative_analysis():
    """ä¸‰ç§è°ƒç”¨æ¨¡å¼å¯¹æ¯”åˆ†æ"""
    test_prompt = "è¯·è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"
    
    print("ğŸ”¬ è°ƒç”¨æ¨¡å¼å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)
    
    # åŒæ­¥è°ƒç”¨æµ‹è¯•
    print("\nğŸ”„ åŒæ­¥è°ƒç”¨æµ‹è¯•:")
    start_time = time.time()
    sync_response = init_model(model_name).invoke(test_prompt)
    sync_time = time.time() - start_time
    
    print(f"æ‰§è¡Œæ—¶é—´: {sync_time:.2f}ç§’")
    print(f"å†…å®¹é•¿åº¦: {len(sync_response.content)}å­—ç¬¦")
    
    # æµå¼è°ƒç”¨æµ‹è¯•
    print("\nğŸŒŠ æµå¼è°ƒç”¨æµ‹è¯•:")
    start_time = time.time()
    stream_content = ""
    for chunk in init_model(model_name).stream([test_prompt]):
        stream_content += chunk.content
    stream_time = time.time() - start_time
    
    print(f"æ‰§è¡Œæ—¶é—´: {stream_time:.2f}ç§’")
    print(f"å†…å®¹é•¿åº¦: {len(stream_content)}å­—ç¬¦")
    
    # æ‰¹é‡è°ƒç”¨æµ‹è¯•
    print("\nğŸ“¦ æ‰¹é‡è°ƒç”¨æµ‹è¯•:")
    start_time = time.time()
    batch_responses = init_model(model_name).batch([test_prompt])
    batch_time = time.time() - start_time
    
    print(f"æ‰§è¡Œæ—¶é—´: {batch_time:.2f}ç§’")
    print(f"å†…å®¹é•¿åº¦: {len(batch_responses[0].content)}å­—ç¬¦")
    
    # æ€§èƒ½å¯¹æ¯”æ€»ç»“
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“:")
    print(f"åŒæ­¥è°ƒç”¨: {sync_time:.2f}ç§’")
    print(f"æµå¼è°ƒç”¨: {stream_time:.2f}ç§’ ({((sync_time-stream_time)/sync_time*100):+.1f}%)")
    print(f"æ‰¹é‡è°ƒç”¨: {batch_time:.2f}ç§’ ({((sync_time-batch_time)/sync_time*100):+.1f}%)")
```

## ğŸ¨ é«˜çº§åº”ç”¨ç¤ºä¾‹

### 1. æ¨¡æ¿åŒ–è°ƒç”¨ç³»ç»Ÿ

```python
class TemplateInvokeSystem:
    def __init__(self, model):
        self.model = init_model(model)
        self.templates = {
            'poem': "è¯·å†™ä¸€é¦–å…³äº{topic}çš„{style}é£æ ¼{type}",
            'explanation': "è¯·è¯¦ç»†è§£é‡Š{subject}ï¼ŒåŒ…æ‹¬{aspects}",
            'translation': "è¯·å°†ä»¥ä¸‹{source_lang}å†…å®¹ç¿»è¯‘æˆ{target_lang}ï¼š{content}",
            'code_review': "è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç çš„è´¨é‡å’Œæ½œåœ¨é—®é¢˜ï¼š{code}"
        }
    
    def invoke_template(self, template_name, **kwargs):
        """ä½¿ç”¨æ¨¡æ¿è¿›è¡Œè°ƒç”¨"""
        if template_name not in self.templates:
            raise ValueError(f"æœªçŸ¥æ¨¡æ¿: {template_name}")
        
        prompt = self.templates[template_name].format(**kwargs)
        return self.model.invoke(prompt).content
    
    def add_template(self, name, template):
        """æ·»åŠ æ–°æ¨¡æ¿"""
        self.templates[name] = template

# ä½¿ç”¨ç¤ºä¾‹
def template_system_demo():
    """æ¨¡æ¿ç³»ç»Ÿæ¼”ç¤º"""
    template_system = TemplateInvokeSystem(model_name)
    
    # è¯—æ­Œåˆ›ä½œ
    poem = template_system.invoke_template(
        'poem',
        topic='ç§‹å¶',
        style='å¤å…¸',
        type='äº”è¨€ç»å¥'
    )
    print("ğŸ‚ ç§‹å¶è¯—:")
    print(poem)
    
    # æŠ€æœ¯è§£é‡Š
    explanation = template_system.invoke_template(
        'explanation',
        subject='æœºå™¨å­¦ä¹ ',
        aspects='åŸºæœ¬æ¦‚å¿µã€ä¸»è¦ç®—æ³•ã€åº”ç”¨åœºæ™¯'
    )
    print("\nğŸ¤– æœºå™¨å­¦ä¹ è§£é‡Š:")
    print(explanation[:200] + "...")
```

### 2. æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

```python
import hashlib
from functools import lru_cache

class IntelligentCache:
    def __init__(self, maxsize=128):
        self.cache = {}
        self.maxsize = maxsize
        self.stats = {'hits': 0, 'misses': 0}
    
    def get_cache_key(self, prompt, model_config):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = f"{prompt}_{str(model_config)}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def cached_invoke(self, model, prompt, **model_params):
        """å¸¦ç¼“å­˜çš„æ™ºèƒ½è°ƒç”¨"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.get_cache_key(prompt, model_params)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            self.stats['hits'] += 1
            print("ğŸ’¾ ç¼“å­˜å‘½ä¸­")
            return self.cache[cache_key]
        
        self.stats['misses'] += 1
        print("ğŸ†• ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè°ƒç”¨")
        
        # æ‰§è¡Œå®é™…è°ƒç”¨
        try:
            model_instance = init_model(model)
            # åº”ç”¨å‚æ•°
            for param, value in model_params.items():
                setattr(model_instance, param, value)
            
            result = model_instance.invoke(prompt)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            if len(self.cache) >= self.maxsize:
                # åˆ é™¤æœ€è€çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
            
            self.cache[cache_key] = result
            return result
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
            raise

# ä½¿ç”¨ç¤ºä¾‹
def caching_demo():
    """ç¼“å­˜ç³»ç»Ÿæ¼”ç¤º"""
    cache = IntelligentCache()
    
    # é‡å¤è°ƒç”¨æµ‹è¯•
    test_prompts = [
        "å†™ä¸€å¥åŠ±å¿—çš„è¯",
        "è§£é‡Šäººå·¥æ™ºèƒ½æ¦‚å¿µ",
        "å†™ä¸€å¥åŠ±å¿—çš„è¯"  # é‡å¤è°ƒç”¨
    ]
    
    for i, prompt in enumerate(test_prompts):
        print(f"\nğŸ“¥ è°ƒç”¨ {i+1}: {prompt}")
        result = cache.cached_invoke(model_name, prompt, temperature=0.7)
        print(f"ğŸ“¤ ç»“æœ: {result.content[:50]}...")
    
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­{cache.stats['hits']}æ¬¡ï¼Œæœªå‘½ä¸­{cache.stats['misses']}æ¬¡")
```

## ğŸ“ æ€»ç»“

åŸºç¡€åŒæ­¥è°ƒç”¨æ˜¯LangChainåº”ç”¨å¼€å‘çš„åŸºçŸ³ï¼š

âœ… **ç®€å•å¯é **ï¼šæœ€å®¹æ˜“ç†è§£å’Œå®ç°çš„è°ƒç”¨æ–¹å¼  
âœ… **åŠŸèƒ½å®Œæ•´**ï¼šæ”¯æŒæ‰€æœ‰åŸºç¡€AIäº¤äº’éœ€æ±‚  
âœ… **è°ƒè¯•å‹å¥½**ï¼šä¾¿äºé—®é¢˜æ’æŸ¥å’Œæ€§èƒ½åˆ†æ  
âœ… **æ‰©å±•æ€§å¼º**ï¼šä¸ºå…¶ä»–é«˜çº§åŠŸèƒ½å¥ å®šåŸºç¡€  

## ğŸ”— ç›¸å…³èµ„æº

- [LangChainå®˜æ–¹æ–‡æ¡£ - Chat Models](https://python.langchain.com/docs/modules/model_io/chat/)
- [OpenAI APIå‚æ•°è¯´æ˜](https://platform.openai.com/docs/api-reference/chat)
- [Python dotenvä½¿ç”¨æŒ‡å—](https://github.com/theskumar/python-dotenv)

---
*æœ¬æ•™ç¨‹è¯¦ç»†è§£æäº†LangChainåŸºç¡€åŒæ­¥è°ƒç”¨çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä¸‹ä¸€æœŸæˆ‘ä»¬å°†æ¢è®¨æ‰¹é‡è°ƒç”¨çš„æ•ˆç‡ä¼˜åŒ–æŠ€å·§ã€‚*