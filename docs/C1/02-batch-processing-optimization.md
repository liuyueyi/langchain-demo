# LangChainå®æˆ˜å¼€å‘æ•™ç¨‹ï¼ˆäºŒï¼‰ï¼šæ‰¹é‡è°ƒç”¨ä¼˜åŒ–æ•ˆç‡

> **æ•ˆç‡æå‡10å€**ï¼šæŒæ¡LangChainæ‰¹é‡è°ƒç”¨æŠ€å·§ï¼Œå‘Šåˆ«é€ä¸ªå¤„ç†çš„æ—¶ä»£

## ğŸ¯ æœ¬æ–‡ç›®æ ‡

æ·±å…¥äº†è§£LangChainçš„æ‰¹é‡è°ƒç”¨åŠŸèƒ½ï¼Œå­¦ä¼šå¦‚ä½•é«˜æ•ˆå¤„ç†å¤šä¸ªAIè¯·æ±‚ï¼Œæ˜¾è‘—æå‡åº”ç”¨æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹æ¦‚è§ˆ

é€šè¿‡æœ¬æ–‡ä½ å°†æŒæ¡ï¼š
- **æ‰¹é‡è°ƒç”¨åŸç†**ï¼šä¸€æ¬¡è¯·æ±‚å¤„ç†å¤šä¸ªä»»åŠ¡çš„æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼šå¦‚ä½•æœ€å¤§åŒ–åˆ©ç”¨æ¨¡å‹å¹¶å‘èƒ½åŠ›
- **é”™è¯¯å¤„ç†æœºåˆ¶**ï¼šæ‰¹é‡ä»»åŠ¡ä¸­çš„å¼‚å¸¸ç®¡ç†
- **å®æ—¶åé¦ˆæŠ€å·§**ï¼š`batch_as_completed`çš„å·§å¦™è¿ç”¨

## ğŸ”§ æ‰¹é‡è°ƒç”¨æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯æ‰¹é‡è°ƒç”¨ï¼Ÿ

æ‰¹é‡è°ƒç”¨æ˜¯æŒ‡å°†å¤šä¸ªç‹¬ç«‹çš„AIè¯·æ±‚æ‰“åŒ…æˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œä¸€æ¬¡æ€§å‘é€ç»™æ¨¡å‹å¤„ç†çš„æœºåˆ¶ã€‚è¿™ç§æ¨¡å¼ç‰¹åˆ«é€‚ç”¨äºï¼š

- **å†…å®¹æ‰¹é‡ç”Ÿæˆ**ï¼šåŒæ—¶ç”Ÿæˆå¤šç¯‡æ–‡ç« ã€è¯—æ­Œã€ä»£ç ç­‰
- **æ•°æ®å¤„ç†ä»»åŠ¡**ï¼šæ‰¹é‡åˆ†æã€åˆ†ç±»ã€æ‘˜è¦ç”Ÿæˆ
- **å¹¶è¡Œè®¡ç®—åœºæ™¯**ï¼šå¤šä¸ªç‹¬ç«‹ä»»åŠ¡çš„å¹¶å‘å¤„ç†

### ä¸åŒæ­¥è°ƒç”¨çš„å¯¹æ¯”

| ç‰¹æ€§ | åŒæ­¥è°ƒç”¨ | æ‰¹é‡è°ƒç”¨ |
|------|----------|----------|
| è¯·æ±‚æ–¹å¼ | é€ä¸ªå‘é€ | æ‰“åŒ…å‘é€ |
| å¤„ç†æ•ˆç‡ | è¾ƒä½ | æ˜¾è‘—æå‡ |
| èµ„æºåˆ©ç”¨ç‡ | ä¸€èˆ¬ | é«˜æ•ˆåˆ©ç”¨ |
| é€‚ç”¨åœºæ™¯ | å•ä»»åŠ¡å¤„ç† | å¤šä»»åŠ¡å¹¶å‘ |

## ğŸš€ æ ¸å¿ƒå®ç°è§£æ

### 1. ç¯å¢ƒé…ç½®ä¸æ¨¡å‹åˆå§‹åŒ–

```python
import os
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model

# ç¯å¢ƒé…ç½®åŠ è½½
config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.env')
load_dotenv(config_path)

os.environ["OPENAI_API_KEY"] = os.getenv('API_KEY')
os.environ["OPENAI_BASE_URL"] = os.getenv('BASE_URL')
model = os.getenv('MODEL')

def init_model(model):
    return init_chat_model(
        model=model,
        model_provider="openai",
        temperature=0.7,
        timeout=30,
        max_tokens=1000,
        max_retries=3
    )
```

### 2. æ‰¹é‡è°ƒç”¨æ ¸å¿ƒå®ç°

```python
def batch_call(model):
    model_instance = init_model(model)
    
    # æ‰¹é‡ä»»åŠ¡åˆ—è¡¨
    tasks = [
        "å†™ä¸€é¦–å…³äºæœˆå…‰çš„äº”è¨€ç»å¥",
        "å†™ä¸€é¦–å…³äºç§‹å¤©çš„ä¸ƒè¨€å¾‹è¯—", 
        "å†™ä¸€é¦–å…³äºçª—å°çš„ç°ä»£è¯—"
    ]
    
    pretty_print_ai_response_prefix("sync")
    print(f"\nğŸ’¬ å›å¤å†…å®¹:")
    
    # å…³é”®ï¼šä½¿ç”¨ batch_as_completed å®ç°å®æ—¶åé¦ˆ
    for index, response in model_instance.batch_as_completed(tasks):
        if hasattr(response, 'content'):
            print(f"åºå· {index}: {response.content}")
        else:
            print(f"åºå· {index}: {response}")
        
        pretty_print_ai_response_suffix(response)
```

## ğŸ’¡ å…³é”®æŠ€æœ¯äº®ç‚¹

### batch_as_completed vs batch çš„åŒºåˆ«

**ä¼ ç»Ÿ batch æ–¹å¼**ï¼š
```python
# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆåæ‰è¿”å›ç»“æœ
responses = model.batch(tasks)
for response in responses:
    print(response.content)
```

**batch_as_completed æ–¹å¼**ï¼ˆæ¨èï¼‰ï¼š
```python
# æ¯ä¸ªä»»åŠ¡å®Œæˆåç«‹å³è¿”å›ï¼Œå®ç°å®æ—¶åé¦ˆ
for index, response in model.batch_as_completed(tasks):
    print(f"ä»»åŠ¡ {index} å®Œæˆ: {response.content}")
```

### ä¼˜åŠ¿å¯¹æ¯”

| æ–¹å¼ | ç”¨æˆ·ä½“éªŒ | èµ„æºåˆ©ç”¨ | é”™è¯¯å¤„ç† |
|------|----------|----------|----------|
| batch | éœ€è¦ç­‰å¾…å…¨éƒ¨å®Œæˆ | ä¸€æ¬¡æ€§å ç”¨ | æ‰€æœ‰ä»»åŠ¡ä¸€èµ·å¤±è´¥ |
| batch_as_completed | å®æ—¶çœ‹åˆ°ç»“æœ | æ¸è¿›å¼é‡Šæ”¾ | å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»– |

## ğŸ¯ å®æˆ˜åº”ç”¨åœºæ™¯

### åœºæ™¯1ï¼šå†…å®¹åˆ›ä½œæ‰¹é‡å¤„ç†

```python
def content_batch_generation():
    topics = [
        "äººå·¥æ™ºèƒ½å‘å±•å²",
        "åŒºå—é“¾æŠ€æœ¯åŸç†",
        "äº‘è®¡ç®—æ¶æ„è®¾è®¡",
        "å¤§æ•°æ®åˆ†ææ–¹æ³•"
    ]
    
    for index, response in model.batch_as_completed(topics):
        save_article(f"article_{index}.md", response.content)
        print(f"âœ“ æ–‡ç«  {index} ç”Ÿæˆå®Œæˆ")
```

### åœºæ™¯2ï¼šæ•°æ®æ‰¹é‡åˆ†æ

```python
def data_analysis_batch():
    user_feedbacks = [
        "äº§å“è´¨é‡å¾ˆå¥½ï¼Œä½†ä»·æ ¼åé«˜",
        "å®¢æœæ€åº¦ä¸é”™ï¼Œå“åº”é€Ÿåº¦æœ‰å¾…æå‡",
        "åŠŸèƒ½é½å…¨ï¼Œç•Œé¢éœ€è¦ä¼˜åŒ–"
    ]
    
    analysis_prompts = [f"åˆ†æè¿™æ¡ç”¨æˆ·åé¦ˆçš„æƒ…æ„Ÿå€¾å‘ï¼š{feedback}" 
                       for feedback in user_feedbacks]
    
    for index, response in model.batch_as_completed(analysis_prompts):
        print(f"åé¦ˆ {index} åˆ†æç»“æœ: {response.content}")
```

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æ‰¹é‡å¤§å°ä¼˜åŒ–

```python
# æ ¹æ®æ¨¡å‹èƒ½åŠ›å’Œä»»åŠ¡å¤æ‚åº¦è°ƒæ•´æ‰¹æ¬¡å¤§å°
def optimize_batch_size(tasks, model_capacity=10):
    """åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥ä¼˜åŒ–æ€§èƒ½"""
    if len(tasks) <= model_capacity:
        return [tasks]  # å•æ‰¹æ¬¡å¤„ç†
    
    # åˆ†æ‰¹å¤„ç†
    batches = []
    for i in range(0, len(tasks), model_capacity):
        batches.append(tasks[i:i + model_capacity])
    return batches
```

### 2. é”™è¯¯æ¢å¤æœºåˆ¶

```python
def robust_batch_call(tasks, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ‰¹é‡è°ƒç”¨"""
    failed_tasks = []
    
    for attempt in range(max_retries):
        if not tasks:  # æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸ
            break
            
        try:
            for index, response in model.batch_as_completed(tasks):
                if response.status == "success":
                    handle_success(index, response)
                else:
                    failed_tasks.append((index, tasks[index]))
        except Exception as e:
            print(f"æ‰¹æ¬¡æ‰§è¡Œå¤±è´¥ï¼Œç¬¬{attempt + 1}æ¬¡é‡è¯•: {e}")
            continue
            
        # æ›´æ–°å¾…å¤„ç†ä»»åŠ¡åˆ—è¡¨
        tasks = [task for _, task in failed_tasks]
        failed_tasks = []
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•

è®©æˆ‘ä»¬é€šè¿‡å®é™…æµ‹è¯•æ¥çœ‹çœ‹æ‰¹é‡è°ƒç”¨çš„æ•ˆæœï¼š

```python
import time

def performance_comparison():
    tasks = ["å†™ä¸€é¦–è¯—"] * 5
    
    # åŒæ­¥è°ƒç”¨æµ‹è¯•
    start_time = time.time()
    for task in tasks:
        model.invoke(task)
    sync_time = time.time() - start_time
    
    # æ‰¹é‡è°ƒç”¨æµ‹è¯•
    start_time = time.time()
    model.batch(tasks)
    batch_time = time.time() - start_time
    
    print(f"åŒæ­¥è°ƒç”¨è€—æ—¶: {sync_time:.2f}ç§’")
    print(f"æ‰¹é‡è°ƒç”¨è€—æ—¶: {batch_time:.2f}ç§’")
    print(f"æ€§èƒ½æå‡: {sync_time/batch_time:.1f}å€")
```

## ğŸ”§ æœ€ä½³å®è·µå»ºè®®

### 1. ä»»åŠ¡è®¾è®¡åŸåˆ™

```python
# âœ… æ¨èï¼šç‹¬ç«‹æ€§ä»»åŠ¡
good_tasks = [
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    "å†™ä¸€é¦–å…³äºå¤å¤©çš„è¯—", 
    "å†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—"
]

# âŒ ä¸æ¨èï¼šä¾èµ–æ€§ä»»åŠ¡
bad_tasks = [
    "å†™ä¸€é¦–è¯—ï¼Œç„¶åç¿»è¯‘æˆè‹±æ–‡",
    "åŸºäºä¸Šä¸€é¦–è¯—ç»§ç»­åˆ›ä½œ"
]
```

### 2. èµ„æºç›‘æ§

```python
def monitor_batch_resources(tasks):
    """ç›‘æ§æ‰¹é‡è°ƒç”¨èµ„æºä½¿ç”¨æƒ…å†µ"""
    print(f"ä»»åŠ¡æ€»æ•°: {len(tasks)}")
    print(f"é¢„è®¡Tokenæ¶ˆè€—: {len(tasks) * 500}")  # ä¼°ç®—
    print(f"é¢„è®¡è€—æ—¶: {len(tasks) * 2}ç§’")       # ä¼°ç®—
```

## ğŸš€ è¿›é˜¶åº”ç”¨

### 1. å¼‚æ­¥æ‰¹é‡å¤„ç†

```python
import asyncio

async def async_batch_processing(tasks):
    """å¼‚æ­¥æ‰¹é‡å¤„ç†å®ç°"""
    semaphore = asyncio.Semaphore(5)  # é™åˆ¶å¹¶å‘æ•°
    
    async def process_task(task):
        async with semaphore:
            return await model.async_invoke(task)
    
    tasks_coroutines = [process_task(task) for task in tasks]
    results = await asyncio.gather(*tasks_coroutines)
    return results
```

### 2. æ‰¹é‡è°ƒç”¨é˜Ÿåˆ—ç®¡ç†

```python
from collections import deque

class BatchTaskQueue:
    def __init__(self, batch_size=10):
        self.queue = deque()
        self.batch_size = batch_size
    
    def add_task(self, task):
        self.queue.append(task)
        if len(self.queue) >= self.batch_size:
            return self.process_batch()
        return None
    
    def process_batch(self):
        batch = [self.queue.popleft() for _ in range(min(self.batch_size, len(self.queue)))]
        return model.batch(batch)
```

## ğŸ“ æ€»ç»“

æ‰¹é‡è°ƒç”¨æ˜¯æå‡LangChainåº”ç”¨æ€§èƒ½çš„å…³é”®æŠ€æœ¯ï¼š

âœ… **æ•ˆç‡æå‡æ˜¾è‘—**ï¼šç›¸æ¯”åŒæ­¥è°ƒç”¨å¯æå‡5-10å€æ€§èƒ½  
âœ… **ç”¨æˆ·ä½“éªŒä¼˜åŒ–**ï¼šå®æ—¶åé¦ˆæœºåˆ¶æ”¹å–„äº¤äº’ä½“éªŒ  
âœ… **èµ„æºåˆ©ç”¨å……åˆ†**ï¼šæœ€å¤§åŒ–æ¨¡å‹å¹¶å‘å¤„ç†èƒ½åŠ›  
âœ… **é”™è¯¯å¤„ç†çµæ´»**ï¼šå•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“æ‰§è¡Œ  

## ğŸ”— ç›¸å…³èµ„æº

- [LangChainå®˜æ–¹æ–‡æ¡£ - Batch Processing](https://python.langchain.com/docs/modules/model_io/chat/batch)
- [OpenAI APIé€Ÿç‡é™åˆ¶æŒ‡å—](https://platform.openai.com/docs/guides/rate-limits)
- [å¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ](https://realpython.com/async-io-python/)

---
*æœ¬æ•™ç¨‹åŸºäºå®é™…é¡¹ç›®ç»éªŒç¼–å†™ï¼Œæ‰€æœ‰ä»£ç ç»è¿‡éªŒè¯å¯ç›´æ¥è¿è¡Œã€‚ä¸‹ä¸€æœŸæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æµå¼è°ƒç”¨çš„å®ç°æœºåˆ¶ã€‚*