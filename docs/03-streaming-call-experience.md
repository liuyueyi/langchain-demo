# LangChainå®æˆ˜å¼€å‘æ•™ç¨‹ï¼ˆä¸‰ï¼‰ï¼šæµå¼è°ƒç”¨ä½“éªŒå‡çº§

> **æ·±å…¥ç†è§£æµå¼è°ƒç”¨**ï¼šLangChainæµå¼è¾“å‡ºçš„æ ¸å¿ƒæœºåˆ¶ä¸æœ€ä½³å®è·µ

## ğŸ¯ æœ¬æ–‡ç›®æ ‡

å…¨é¢è§£æLangChainæµå¼è°ƒç”¨çš„å®ç°åŸç†ï¼ŒæŒæ¡å®æ—¶è¾“å‡ºæŠ€æœ¯ï¼Œæ‰“é€ æ›´è‡ªç„¶çš„äººæœºäº¤äº’ä½“éªŒã€‚

## ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹æ¦‚è§ˆ

é€šè¿‡æœ¬æ–‡ä½ å°†æ·±å…¥ç†è§£ï¼š
- **æµå¼è°ƒç”¨å·¥ä½œæœºåˆ¶**ï¼šé€tokenè¾“å‡ºçš„æŠ€æœ¯åŸç†
- **ç”¨æˆ·ä½“éªŒä¼˜åŒ–**ï¼šå®æ—¶åé¦ˆvså®Œæ•´å“åº”çš„æƒè¡¡
- **Tokenä½¿ç”¨ç›‘æ§**ï¼šæµå¼åœºæ™¯ä¸‹çš„èµ„æºç»Ÿè®¡æ–¹æ³•
- **å¼‚å¸¸å¤„ç†ç­–ç•¥**ï¼šç½‘ç»œä¸­æ–­æ—¶çš„ä¼˜é›…é™çº§

## ğŸ” æµå¼è°ƒç”¨æ ¸å¿ƒæŠ€æœ¯è§£æ

### ä»€ä¹ˆæ˜¯æµå¼è°ƒç”¨ï¼Ÿ

æµå¼è°ƒç”¨æ˜¯ä¸€ç§é€tokenè¿”å›AIç”Ÿæˆç»“æœçš„æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥çœ‹åˆ°æ¨¡å‹"è¾¹æ€è€ƒè¾¹è¾“å‡º"çš„è¿‡ç¨‹ï¼Œå°±åƒçœŸäººæ‰“å­—ä¸€æ ·ã€‚

### å·¥ä½œæœºåˆ¶å¯¹æ¯”

| è°ƒç”¨æ–¹å¼ | æ•°æ®ä¼ è¾“ | ç”¨æˆ·ä½“éªŒ | èµ„æºæ¶ˆè€— |
|----------|----------|----------|----------|
| åŒæ­¥è°ƒç”¨ | ä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç»“æœ | ç­‰å¾…æ—¶é—´é•¿ | ä½ |
| æµå¼è°ƒç”¨ | é€tokenå®æ—¶è¿”å› | å³æ—¶åé¦ˆ | ä¸­ç­‰ |
| æ‰¹é‡è°ƒç”¨ | å¤šä»»åŠ¡å¹¶å‘å¤„ç† | é«˜æ•ˆä½†éå®æ—¶ | é«˜ |

## ğŸš€ æ ¸å¿ƒå®ç°è¯¦è§£

### 1. åŸºç¡€æµå¼è°ƒç”¨å®ç°

```python
def stream_call(model):
    model_instance = init_model(model)
    
    pretty_print_ai_response_prefix("stream")
    
    # å…³é”®ï¼šä½¿ç”¨ stream() æ–¹æ³•è€Œé invoke()
    token_usage = None
    for chunk in model_instance.stream("è¯·å†™ä¸€é¦–å…³äºé¢œè‰²çš„äº”è¨€ç»å¥"):
        # å®æ—¶è¾“å‡ºæ¯ä¸ªchunkçš„å†…å®¹
        if chunk.usage_metadata:
            token_usage = chunk.usage_metadata
        print(chunk.content, end='', flush=True)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    pretty_print_ai_response_suffix(chunk, token_usage)
```

### 2. æµå¼è°ƒç”¨çš„å…³é”®å·®å¼‚

**ä¸åŒæ­¥è°ƒç”¨çš„æ ¸å¿ƒåŒºåˆ«**ï¼š

```python
# åŒæ­¥è°ƒç”¨ - ç­‰å¾…å®Œæ•´ç»“æœ
response = model.invoke("å†™ä¸€é¦–è¯—")
print(response.content)  # ä¸€æ¬¡æ€§è¾“å‡ºå…¨éƒ¨å†…å®¹

# æµå¼è°ƒç”¨ - å®æ—¶è¾“å‡º
for chunk in model.stream("å†™ä¸€é¦–è¯—"):
    print(chunk.content, end='')  # é€å­—ç¬¦è¾“å‡º
```

## ğŸ’¡ æŠ€æœ¯å®ç°è¦ç‚¹

### 1. å®æ—¶è¾“å‡ºæ§åˆ¶

```python
def enhanced_stream_output(prompt, model):
    """å¢å¼ºç‰ˆæµå¼è¾“å‡ºï¼Œæ”¯æŒæ›´å¤šæ§åˆ¶é€‰é¡¹"""
    full_response = ""
    token_stats = {"input": 0, "output": 0}
    
    print("ğŸ¤– AIæ­£åœ¨æ€è€ƒä¸­...")
    print("-" * 50)
    
    try:
        for chunk in model.stream(prompt):
            # å®æ—¶è¾“å‡ºå†…å®¹
            if hasattr(chunk, 'content') and chunk.content:
                print(chunk.content, end='', flush=True)
                full_response += chunk.content
            
            # æ”¶é›†tokenç»Ÿè®¡
            if hasattr(chunk, 'usage_metadata'):
                usage = chunk.usage_metadata
                if usage:
                    token_stats["input"] = usage.get("input_tokens", 0)
                    token_stats["output"] = usage.get("output_tokens", 0)
                
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†æµå¼è¾“å‡º")
    except Exception as e:
        print(f"\n\nâŒ æµå¼è°ƒç”¨å‡ºé”™: {e}")
    
    return full_response, token_stats
```

### 2. Tokenä½¿ç”¨ç›‘æ§

```python
def monitor_token_usage_during_stream(model):
    """æµå¼è°ƒç”¨ä¸­çš„å®æ—¶Tokenç›‘æ§"""
    total_tokens = 0
    chunks_received = 0
    
    for chunk in model.stream("è¯·è¯¦ç»†è§£é‡Šé‡å­è®¡ç®—åŸç†"):
        chunks_received += 1
        
        if chunk.usage_metadata:
            current_total = chunk.usage_metadata.get('total_tokens', 0)
            if current_total > total_tokens:
                total_tokens = current_total
                print(f"\nğŸ“Š Tokenè¿›åº¦: {total_tokens} tokens (å·²æ¥æ”¶{chunks_received}ä¸ªç‰‡æ®µ)")
    
    print(f"\nâœ… æœ€ç»ˆç»Ÿè®¡: æ€»å…±ä½¿ç”¨ {total_tokens} tokens")
```

## ğŸ¯ å®æˆ˜åº”ç”¨åœºæ™¯

### åœºæ™¯1ï¼šé•¿æ–‡æœ¬ç”Ÿæˆ

> æºç è§ `basic01-model/scene/StreamScene.py`

```python
def long_text_streaming():
    """é•¿æ–‡æœ¬æµå¼ç”Ÿæˆç¤ºä¾‹"""
    long_prompt = """è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„è¯¦ç»†æ–‡ç« ï¼Œ
    åŒ…æ‹¬æŠ€æœ¯è¶‹åŠ¿ã€ç¤¾ä¼šå½±å“ã€ä¼¦ç†è€ƒé‡ç­‰æ–¹é¢ï¼Œè‡³å°‘1000å­—ã€‚"""
    
    print("ğŸ“ å¼€å§‹ç”Ÿæˆé•¿ç¯‡æ–‡ç« ...")
    full_text, stats = enhanced_stream_output(long_prompt, model)
    
    print(f"\n\nğŸ“‹ æ–‡ç« ç»Ÿè®¡:")
    print(f"å­—æ•°: {len(full_text)} å­—ç¬¦")
    print(f"Tokenæ¶ˆè€—: è¾“å…¥{stats['input']}, è¾“å‡º{stats['output']}")
    print(f"ç”Ÿæˆæ•ˆç‡: {stats['output']/len(full_text):.2f} tokens/å­—ç¬¦")
```

### åœºæ™¯2ï¼šä»£ç ç”Ÿæˆå®æ—¶é¢„è§ˆ

```python
def code_generation_stream():
    """ä»£ç ç”Ÿæˆçš„æµå¼é¢„è§ˆ"""
    code_prompt = "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ï¼Œå¹¶æ·»åŠ è¯¦ç»†æ³¨é‡Š"
    
    print("ğŸ’» æ­£åœ¨ç”Ÿæˆä»£ç ...")
    print("=" * 60)
    
    for chunk in model.stream(code_prompt):
        # ä»£ç é«˜äº®æ•ˆæœæ¨¡æ‹Ÿ
        if chunk.content:
            if 'def' in chunk.content or 'class' in chunk.content:
                print(f"\033[94m{chunk.content}\033[0m", end='')  # è“è‰²
            elif 'import' in chunk.content:
                print(f"\033[92m{chunk.content}\033[0m", end='')  # ç»¿è‰²
            elif '#' in chunk.content:
                print(f"\033[93m{chunk.content}\033[0m", end='')  # é»„è‰²
            else:
                print(chunk.content, end='')
    
    print("\n" + "=" * 60)
    print("âœ… ä»£ç ç”Ÿæˆå®Œæˆ!")
```

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. ç¼“å†²åŒºä¼˜åŒ–

```python
def buffered_stream_output(prompt, model, buffer_size=10):
    """å¸¦ç¼“å†²çš„æµå¼è¾“å‡ºï¼Œå‡å°‘é¢‘ç¹æ‰“å°"""
    buffer = ""
    char_count = 0
    
    for chunk in model.stream(prompt):
        if chunk.content:
            buffer += chunk.content
            char_count += len(chunk.content)
            
            # è¾¾åˆ°ç¼“å†²åŒºå¤§å°æˆ–é‡åˆ°å¥å­ç»“æŸç¬¦æ—¶è¾“å‡º
            if len(buffer) >= buffer_size or chunk.content in '.!?ã€‚ï¼ï¼Ÿ':
                print(buffer, end='', flush=True)
                buffer = ""
    
    # è¾“å‡ºå‰©ä½™ç¼“å†²å†…å®¹
    if buffer:
        print(buffer, end='', flush=True)
```

### 2. è‡ªé€‚åº”æµé€Ÿæ§åˆ¶

```python
import time

def adaptive_stream_speed(prompt, model, target_wpm=200):
    """è‡ªé€‚åº”æµé€Ÿæ§åˆ¶ï¼Œæ¨¡æ‹Ÿäººå·¥æ‰“å­—é€Ÿåº¦"""
    chars_per_second = target_wpm * 5 / 60  # æ¯ç§’å­—ç¬¦æ•°
    
    for chunk in model.stream(prompt):
        if chunk.content:
            print(chunk.content, end='', flush=True)
            # æ ¹æ®å†…å®¹é•¿åº¦å»¶è¿Ÿè¾“å‡º
            delay = len(chunk.content) / chars_per_second
            time.sleep(delay)
```

## ğŸ›¡ï¸ å¼‚å¸¸å¤„ç†ä¸å®¹é”™

### 1. ç½‘ç»œä¸­æ–­å¤„ç†

```python
def resilient_stream_call(prompt, model, max_retries=3):
    """å…·æœ‰é‡è¯•æœºåˆ¶çš„æµå¼è°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            print(f"ğŸ“¡ å°è¯•ç¬¬ {attempt + 1} æ¬¡è¿æ¥...")
            full_response = ""
            
            for chunk in model.stream(prompt):
                if chunk.content:
                    print(chunk.content, end='', flush=True)
                    full_response += chunk.content
                    
            return full_response
            
        except Exception as e:
            print(f"\nâŒ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                print("â³ ç­‰å¾…é‡è¯•...")
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            else:
                print("ğŸ”„ åˆ‡æ¢åˆ°åŒæ­¥è°ƒç”¨æ¨¡å¼...")
                return model.invoke(prompt).content
```

### 2. æ–­ç‚¹ç»­ä¼ æœºåˆ¶

```python
def resumeable_stream_call(prompt, model, checkpoint_file="stream_checkpoint.txt"):
    """æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æµå¼è°ƒç”¨"""
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ–­ç‚¹
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            completed_content = f.read()
        print(f"ğŸ” ä»æ–­ç‚¹æ¢å¤: å·²å®Œæˆ {len(completed_content)} å­—ç¬¦")
    else:
        completed_content = ""
    
    try:
        for chunk in model.stream(prompt):
            if chunk.content:
                print(chunk.content, end='', flush=True)
                completed_content += chunk.content
                
                # å®šæœŸä¿å­˜æ–­ç‚¹
                if len(completed_content) % 100 == 0:
                    with open(checkpoint_file, 'w') as f:
                        f.write(completed_content)
                        
    finally:
        # æ¸…ç†æ–­ç‚¹æ–‡ä»¶
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
```

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸åˆ†æ

### 1. å®æ—¶æ€§èƒ½æŒ‡æ ‡

```python
def performance_metrics_stream(prompt, model):
    """æµå¼è°ƒç”¨æ€§èƒ½æŒ‡æ ‡ç›‘æ§"""
    import time
    
    start_time = time.time()
    token_count = 0
    chunk_count = 0
    
    print("ğŸš€ å¼€å§‹æ€§èƒ½ç›‘æ§...")
    
    for chunk in model.stream(prompt):
        chunk_count += 1
        if chunk.content:
            token_count += len(chunk.content.split())
            print(chunk.content, end='', flush=True)
            
        # æ¯10ä¸ªchunkè¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
        if chunk_count % 10 == 0:
            elapsed = time.time() - start_time
            tokens_per_second = token_count / elapsed if elapsed > 0 else 0
            print(f"\nğŸ“Š å®æ—¶ç»Ÿè®¡: {tokens_per_second:.1f} tokens/sec")
    
    total_time = time.time() - start_time
    print(f"\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½:")
    print(f"æ€»æ—¶é—´: {total_time:.2f} ç§’")
    print(f"å¹³å‡é€Ÿåº¦: {token_count/total_time:.1f} tokens/sec")
    print(f"æ€»chunkæ•°: {chunk_count}")
```

### 2. ç”¨æˆ·ä½“éªŒè¯„ä¼°

```python
def ux_evaluation_stream(prompt, model):
    """ç”¨æˆ·ä½“éªŒè¯„ä¼°æŒ‡æ ‡"""
    metrics = {
        'first_token_latency': 0,  # é¦–tokenå»¶è¿Ÿ
        'average_chunk_interval': 0,  # å¹³å‡chunké—´éš”
        'total_generation_time': 0,  # æ€»ç”Ÿæˆæ—¶é—´
        'content_fluency': 0  # å†…å®¹æµç•…åº¦
    }
    
    start_time = time.time()
    first_token_time = None
    chunk_times = []
    
    for chunk in model.stream(prompt):
        current_time = time.time()
        
        if first_token_time is None:
            first_token_time = current_time
            metrics['first_token_latency'] = first_token_time - start_time
            
        chunk_times.append(current_time)
        print(chunk.content, end='', flush=True)
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    metrics['total_generation_time'] = time.time() - start_time
    
    if len(chunk_times) > 1:
        intervals = [chunk_times[i] - chunk_times[i-1] for i in range(1, len(chunk_times))]
        metrics['average_chunk_interval'] = sum(intervals) / len(intervals)
    
    # ç®€å•çš„æµç•…åº¦è¯„ä¼°ï¼ˆåŸºäºchunké—´éš”çš„ä¸€è‡´æ€§ï¼‰
    if len(intervals) > 1:
        variance = sum((x - metrics['average_chunk_interval']) ** 2 for x in intervals) / len(intervals)
        metrics['content_fluency'] = 1 / (1 + variance)  # æ–¹å·®è¶Šå°ï¼Œæµç•…åº¦è¶Šé«˜
    
    return metrics
```

## ğŸ¨ é«˜çº§åº”ç”¨ç¤ºä¾‹

### 1. å¤šæ¨¡æ€æµå¼è¾“å‡º

```python
def multimodal_stream_output(text_prompt, image_generator=None):
    """ç»“åˆæ–‡æœ¬å’Œå›¾åƒçš„æµå¼è¾“å‡º"""
    print("ğŸ¨ å¼€å§‹å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆ...")
    
    # æ–‡æœ¬æµå¼è¾“å‡º
    text_content = ""
    for chunk in model.stream(text_prompt):
        if chunk.content:
            print(chunk.content, end='', flush=True)
            text_content += chunk.content
    
    # å¦‚æœæœ‰å›¾åƒç”Ÿæˆå™¨ï¼ŒåŒæ—¶ç”Ÿæˆç›¸å…³å›¾ç‰‡
    if image_generator:
        print("\nğŸ–¼ï¸  æ­£åœ¨ç”Ÿæˆç›¸å…³å›¾åƒ...")
        image_url = image_generator.generate(text_content[:100])  # åŸºäºå‰100å­—ç¬¦
        print(f"å›¾åƒç”Ÿæˆå®Œæˆ: {image_url}")
```

### 2. äº¤äº’å¼æµå¼å¯¹è¯

```python
def interactive_stream_dialogue():
    """äº¤äº’å¼æµå¼å¯¹è¯ç³»ç»Ÿ"""
    print("ğŸ’¬ æ¬¢è¿ä½¿ç”¨æµå¼å¯¹è¯ç³»ç»Ÿï¼è¾“å…¥ 'quit' é€€å‡º")
    
    conversation_history = []
    
    while True:
        user_input = input("\nğŸ‘¤ ä½ è¯´: ")
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("ğŸ‘‹ å†è§ï¼")
            break
            
        conversation_history.append(("user", user_input))
        
        # æ„é€ å¯¹è¯ä¸Šä¸‹æ–‡
        full_prompt = "\n".join([f"{role}: {content}" for role, content in conversation_history])
        
        print("ğŸ¤– AI: ", end='')
        ai_response = ""
        
        for chunk in model.stream(full_prompt + "\nassistant:"):
            if chunk.content:
                print(chunk.content, end='', flush=True)
                ai_response += chunk.content
        
        conversation_history.append(("assistant", ai_response))
        print()  # æ¢è¡Œ
```

## ğŸ“ æ€»ç»“

æµå¼è°ƒç”¨ä¸ºLangChainåº”ç”¨å¸¦æ¥äº†é©å‘½æ€§çš„ç”¨æˆ·ä½“éªŒæå‡ï¼š

âœ… **å®æ—¶åé¦ˆ**ï¼šç”¨æˆ·å¯ä»¥ç«‹å³çœ‹åˆ°ç”Ÿæˆè¿‡ç¨‹  
âœ… **è‡ªç„¶äº¤äº’**ï¼šæ¨¡æ‹Ÿäººç±»æ€è€ƒå’Œè¡¨è¾¾çš„æ–¹å¼  
âœ… **èµ„æºé€æ˜**ï¼šå®æ—¶ç›‘æ§Tokenä½¿ç”¨æƒ…å†µ  
âœ… **å®¹é”™æ€§å¼º**ï¼šæ”¯æŒä¸­æ–­æ¢å¤å’Œä¼˜é›…é™çº§  

## ğŸ”— ç›¸å…³èµ„æº

- [LangChain Stream Documentation](https://python.langchain.com/docs/modules/model_io/chat/streaming)
- [Server-Sent Events è§„èŒƒ](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [WebSocket vs HTTP Streaming](https://www.smashingmagazine.com/2018/02/websocket-api/)

---
*æœ¬æ•™ç¨‹æ·±å…¥è§£æäº†æµå¼è°ƒç”¨çš„æ ¸å¿ƒæœºåˆ¶ã€‚ä¸‹ä¸€æœŸæˆ‘ä»¬å°†æ¢ç´¢å¤šè½®å¯¹è¯çš„å®ç°æŠ€å·§ã€‚*