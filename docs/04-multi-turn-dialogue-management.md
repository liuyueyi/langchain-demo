# LangChainå®æˆ˜å¼€å‘æ•™ç¨‹ï¼ˆå››ï¼‰ï¼šå¤šè½®å¯¹è¯çŠ¶æ€ç®¡ç†

> **è§£å†³å¤æ‚å¯¹è¯åœºæ™¯**ï¼šæŒæ¡LangChainå¤šè½®å¯¹è¯çš„æ ¸å¿ƒæŠ€æœ¯ä¸æœ€ä½³å®è·µ

## ğŸ¯ æœ¬æ–‡ç›®æ ‡

æ·±å…¥æ¢è®¨LangChainå¤šè½®å¯¹è¯çš„å®ç°æœºåˆ¶ï¼Œå­¦ä¼šç®¡ç†å¯¹è¯çŠ¶æ€ã€å¤„ç†ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œæ„å»ºæ™ºèƒ½çš„è¿ç»­å¯¹è¯ç³»ç»Ÿã€‚

## ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹æ¦‚è§ˆ

é€šè¿‡æœ¬æ–‡ä½ å°†æŒæ¡ï¼š
- **å¯¹è¯çŠ¶æ€ç®¡ç†**ï¼šå¦‚ä½•ç»´æŠ¤å¤šè½®å¯¹è¯çš„å†å²è®°å½•
- **ä¸Šä¸‹æ–‡å¤„ç†æŠ€å·§**ï¼šæœ‰æ•ˆåˆ©ç”¨å†å²ä¿¡æ¯æå‡å¯¹è¯è´¨é‡
- **æ¶ˆæ¯ç±»å‹åŒºåˆ†**ï¼šSystem/Human/AIæ¶ˆæ¯çš„æ­£ç¡®ä½¿ç”¨
- **å†…å­˜ä¼˜åŒ–ç­–ç•¥**ï¼šé˜²æ­¢ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´çš„æ€§èƒ½é—®é¢˜

## ğŸ” å¤šè½®å¯¹è¯æ ¸å¿ƒæŠ€æœ¯è§£æ

### ä»€ä¹ˆæ˜¯å¤šè½®å¯¹è¯ï¼Ÿ

å¤šè½®å¯¹è¯æ˜¯æŒ‡AIèƒ½å¤Ÿè®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œå¹¶åŸºäºå†å²ä¸Šä¸‹æ–‡è¿›è¡Œè¿è´¯å›åº”çš„äº¤äº’æ¨¡å¼ã€‚è¿™æ˜¯å®ç°çœŸæ­£æ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„åŸºçŸ³ã€‚

### æ ¸å¿ƒæŒ‘æˆ˜

1. **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šå¦‚ä½•æœ‰æ•ˆå­˜å‚¨å’Œæ£€ç´¢å¯¹è¯å†å²
2. **çŠ¶æ€ç»´æŠ¤**ï¼šä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œä¸€è‡´æ€§
3. **å†…å­˜æ§åˆ¶**ï¼šé˜²æ­¢å†å²ä¿¡æ¯æ— é™å¢é•¿
4. **è§’è‰²åŒºåˆ†**ï¼šæ­£ç¡®è¯†åˆ«ä¸åŒå‚ä¸è€…çš„å‘è¨€

## ğŸš€ ä¸¤ç§å®ç°æ–¹å¼å¯¹æ¯”

### æ–¹å¼ä¸€ï¼šJSONæ ¼å¼å¯¹è¯å†å²

```python
def multi_turn_invoke_json(model):
    """ä½¿ç”¨JSONæ ¼å¼ç®¡ç†å¯¹è¯å†å²"""
    model_instance = init_model(model)
    
    # JSONæ ¼å¼çš„å¯¹è¯å†å²
    conversation = [
        {
            "role": "system", 
            "content": "ä½ ç°åœ¨æ‰®æ¼”ç››å”æœ€è‘—åçš„å¤§è¯—äººæç™½ï¼Œä»¥ç‹‚æ”¾ä¸ç¾ã€é£˜é€¸æ¢¦å¹»ã€å¤§æ°”ç£…ç¤´çš„é£æ ¼è‘—ç§°"
        },
        {
            "role": "user", 
            "content": "è¯·å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜æœˆå…‰çš„å¤è¯—"
        },
        {
            "role": "assistant", 
            "content": """ã€Šæ˜æœˆå…‰èµ‹ã€‹
é’å¤©è£‚é•œè½ä¹ç§‹ï¼Œå†°é­„åˆæ‚¬æ»¡ç¥å·ã€‚
æ¬²å€Ÿé“¶æ²³æ–ŸåŒ—æ–—ï¼Œé†‰å€¾ç‰å£¶ç™½ç‰ç§‹ã€‚
æ¸…è¾‰æ¼«æ´’å¦‚ç§‹éœœåˆƒï¼Œç¢å½±å¾˜å¾Šä¼¼å¤œçœ¸ã€‚
é†‰èˆå¹¿å¯’å®«é˜™å¤–ï¼Œæ‰¶æ‘‡ç›´ä¸Šç ´è‹ç©¹ã€‚
æ˜æœˆç…§æˆ‘æ„æœªå°½ï¼Œä¸”é‚€æ¸…è¾‰é†‰å¿ƒç”°ã€‚"""
        },
        {
            "role": "user", 
            "content": "æˆ‘å¸Œæœ›åœ¨ä¸Šé¢çš„è¿”å›ä¸­ï¼Œæ·»åŠ ä¸€äº›å…³äºä»™äººã€ä¾ å®¢çš„å†…å®¹"
        }
    ]
    
    response = model_instance.invoke(conversation)
    pretty_print_ai_response(response)
```

### æ–¹å¼äºŒï¼šMessageç±»å¯¹è¯å†å²ï¼ˆæ¨èï¼‰

```python
def multi_turn_invoke_messages(model):
    """ä½¿ç”¨LangChain Messageç±»ç®¡ç†å¯¹è¯å†å²"""
    model_instance = init_model(model)
    
    # ä½¿ç”¨ä¸“é—¨çš„æ¶ˆæ¯ç±»ï¼Œä»£ç æ›´æ¸…æ™°æ˜“è¯»
    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
    
    conversation = [
        SystemMessage("ä½ ç°åœ¨æ‰®æ¼”ç››å”æœ€è‘—åçš„å¤§è¯—äººæç™½ï¼Œä»¥ç‹‚æ”¾ä¸ç¾ã€é£˜é€¸æ¢¦å¹»ã€å¤§æ°”ç£…ç¤´çš„é£æ ¼è‘—ç§°"),
        HumanMessage("è¯·å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜æœˆå…‰çš„å¤è¯—"),
        AIMessage("""ã€Šæ˜æœˆå…‰èµ‹ã€‹
é’å¤©è£‚é•œè½ä¹ç§‹ï¼Œå†°é­„åˆæ‚¬æ»¡ç¥å·ã€‚
æ¬²å€Ÿé“¶æ²³æ–ŸåŒ—æ–—ï¼Œé†‰å€¾ç‰å£¶ç™½ç‰ç§‹ã€‚
æ¸…è¾‰æ¼«æ´’å¦‚ç§‹éœœåˆƒï¼Œç¢å½±å¾˜å¾Šä¼¼å¤œçœ¸ã€‚
é†‰èˆå¹¿å¯’å®«é˜™å¤–ï¼Œæ‰¶æ‘‡ç›´ä¸Šç ´è‹ç©¹ã€‚
æ˜æœˆç…§æˆ‘æ„æœªå°½ï¼Œä¸”é‚€æ¸…è¾‰é†‰å¿ƒç”°ã€‚"""),
        HumanMessage("æˆ‘å¸Œæœ›åœ¨ä¸Šé¢çš„è¿”å›ä¸­ï¼Œæ·»åŠ ä¸€äº›å…³äºä»™äººã€ä¾ å®¢çš„å†…å®¹")
    ]
    
    response = model_instance.invoke(conversation)
    pretty_print_ai_response(response)
```

## ğŸ’¡ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

### 1. æ¶ˆæ¯ç±»å‹è¯¦è§£

```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, FunctionMessage

# ç³»ç»Ÿæ¶ˆæ¯ - è®¾å®šAIçš„è¡Œä¸ºå‡†åˆ™
system_msg = SystemMessage("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®ï¼Œå–„äºç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µ")

# äººç±»æ¶ˆæ¯ - ç”¨æˆ·çš„è¾“å…¥
human_msg = HumanMessage("è¯·è§£é‡Šä»€ä¹ˆæ˜¯åŒºå—é“¾ï¼Ÿ")

# AIæ¶ˆæ¯ - æ¨¡å‹çš„å›å¤
ai_msg = AIMessage("åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯...")

# å‡½æ•°æ¶ˆæ¯ - å·¥å…·è°ƒç”¨ç»“æœï¼ˆåç»­ç« èŠ‚è¯¦è¿°ï¼‰
function_msg = FunctionMessage(name="get_weather", content='{"temperature": 25, "condition": "æ™´å¤©"}')
```

### 2. å¯¹è¯å†å²ç®¡ç†å™¨

```python
class ConversationManager:
    def __init__(self, model, max_history=10):
        self.model = init_model(model)
        self.history = []
        self.max_history = max_history
    
    def add_message(self, message):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²è®°å½•"""
        self.history.append(message)
        # é™åˆ¶å†å²é•¿åº¦ï¼Œé˜²æ­¢contextè¿‡é•¿
        if len(self.history) > self.max_history:
            self.history = self.history[-self.max_history:]
    
    def get_context(self):
        """è·å–å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡"""
        return self.history.copy()
    
    def chat(self, user_input):
        """è¿›è¡Œä¸€è½®å¯¹è¯"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        user_message = HumanMessage(user_input)
        self.add_message(user_message)
        
        # è·å–AIå›å¤
        response = self.model.invoke(self.get_context())
        ai_message = AIMessage(response.content)
        self.add_message(ai_message)
        
        return response.content
    
    def reset(self):
        """é‡ç½®å¯¹è¯å†å²"""
        self.history = []

# ä½¿ç”¨ç¤ºä¾‹
manager = ConversationManager(model)
print(manager.chat("ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜"))
print(manager.chat("ä½ èƒ½å‘Šè¯‰æˆ‘ä»Šå¤©çš„å¤©æ°”å—ï¼Ÿ"))
```

## ğŸ¯ å®æˆ˜åº”ç”¨åœºæ™¯

### åœºæ™¯1ï¼šè§’è‰²æ‰®æ¼”å¯¹è¯ç³»ç»Ÿ

```python
class RolePlayConversation:
    def __init__(self, character_prompt, model):
        self.model = init_model(model)
        self.character_prompt = character_prompt
        self.conversation = [SystemMessage(character_prompt)]
    
    def speak(self, user_message):
        """ç”¨æˆ·å‘è¨€"""
        self.conversation.append(HumanMessage(user_message))
        
        response = self.model.invoke(self.conversation)
        ai_response = AIMessage(response.content)
        self.conversation.append(ai_response)
        
        return response.content
    
    def get_character_state(self):
        """è·å–è§’è‰²å½“å‰çŠ¶æ€"""
        # å¯ä»¥é€šè¿‡ç‰¹æ®Šæç¤ºè¯è¯¢é—®è§’è‰²çŠ¶æ€
        state_prompt = HumanMessage("è¯·ç®€è¦æè¿°ä½ ç°åœ¨çš„å¿ƒæƒ…å’ŒçŠ¶æ€")
        temp_conversation = self.conversation + [state_prompt]
        state_response = self.model.invoke(temp_conversation)
        return state_response.content

# ä½¿ç”¨ç¤ºä¾‹
role_play = RolePlayConversation(
    "ä½ æ˜¯ä¸€ä½å¤ä»£ä¹¦é™¢çš„å…ˆç”Ÿï¼Œåšå­¦å¤šæ‰ï¼Œè¯´è¯æ–‡é›…æœ‰ç¤¼", 
    model
)
print(role_play.speak("å…ˆç”Ÿï¼Œè¯·é—®å¦‚ä½•ä¿®èº«é½å®¶ï¼Ÿ"))
print(role_play.speak("é‚£æ²»å›½å¹³å¤©ä¸‹å‘¢ï¼Ÿ"))
```

### åœºæ™¯2ï¼šæŠ€æœ¯æ”¯æŒå¯¹è¯æœºå™¨äºº

```python
class TechSupportBot:
    def __init__(self, model):
        self.model = init_model(model)
        self.session_history = []
        self.current_issue = None
    
    def start_session(self, user_problem):
        """å¼€å§‹æŠ€æœ¯æ”¯æŒä¼šè¯"""
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æ”¯æŒå·¥ç¨‹å¸ˆï¼Œå–„äºï¼š
        1. ä»”ç»†å€¾å¬ç”¨æˆ·é—®é¢˜
        2. é€æ­¥å¼•å¯¼ç”¨æˆ·æä¾›è¯¦ç»†ä¿¡æ¯
        3. æä¾›æ¸…æ™°çš„è§£å†³æ–¹æ¡ˆ
        4. ç¡®è®¤é—®é¢˜æ˜¯å¦è§£å†³"""
        
        self.session_history = [
            SystemMessage(system_prompt),
            HumanMessage(f"ç”¨æˆ·æŠ¥å‘Šé—®é¢˜ï¼š{user_problem}")
        ]
        
        self.current_issue = user_problem
        response = self.model.invoke(self.session_history)
        self.session_history.append(AIMessage(response.content))
        return response.content
    
    def continue_session(self, user_response):
        """ç»§ç»­ä¼šè¯"""
        self.session_history.append(HumanMessage(user_response))
        response = self.model.invoke(self.session_history)
        self.session_history.append(AIMessage(response.content))
        return response.content
    
    def summarize_session(self):
        """æ€»ç»“ä¼šè¯"""
        summary_prompt = HumanMessage("""è¯·æ€»ç»“æœ¬æ¬¡æŠ€æœ¯æ”¯æŒä¼šè¯ï¼š
        1. ç”¨æˆ·æœ€åˆçš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
        2. æˆ‘ä»¬é‡‡å–äº†å“ªäº›è§£å†³æ­¥éª¤ï¼Ÿ
        3. é—®é¢˜æœ€ç»ˆæ˜¯å¦å¾—åˆ°è§£å†³ï¼Ÿ""")
        
        temp_history = self.session_history + [summary_prompt]
        summary = self.model.invoke(temp_history)
        return summary.content

# ä½¿ç”¨ç¤ºä¾‹
support_bot = TechSupportBot(model)
print(support_bot.start_session("æˆ‘çš„ç”µè„‘å¼€æœºå¾ˆæ…¢"))
print(support_bot.continue_session("å¤§æ¦‚éœ€è¦2åˆ†é’Ÿæ‰èƒ½è¿›å…¥æ¡Œé¢"))
```

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯

```python
def compress_conversation_history(history, max_tokens=2000):
    """å‹ç¼©å¯¹è¯å†å²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯"""
    if not history:
        return history
    
    # ç®€å•çš„é•¿åº¦é™åˆ¶ç­–ç•¥
    total_tokens = sum(len(msg.content.split()) for msg in history if hasattr(msg, 'content'))
    
    if total_tokens <= max_tokens:
        return history
    
    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„å‡ è½®å¯¹è¯
    compressed = []
    
    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    for msg in history:
        if isinstance(msg, SystemMessage):
            compressed.append(msg)
    
    # ä¿ç•™æœ€è¿‘çš„å¯¹è¯è½®æ¬¡
    recent_messages = [msg for msg in history if not isinstance(msg, SystemMessage)][-6:]  # æœ€è¿‘3è½®å¯¹è¯
    compressed.extend(recent_messages)
    
    return compressed

# ä½¿ç”¨ç¤ºä¾‹
long_history = [SystemMessage("ä½ æ˜¯åŠ©æ‰‹")] + [HumanMessage(f"é—®é¢˜{i}") for i in range(20)]
compressed = compress_conversation_history(long_history)
print(f"å‹ç¼©å‰: {len(long_history)} æ¡æ¶ˆæ¯")
print(f"å‹ç¼©å: {len(compressed)} æ¡æ¶ˆæ¯")
```

### 2. æ™ºèƒ½æ‘˜è¦æœºåˆ¶

```python
def create_conversation_summary(model, history):
    """ä¸ºé•¿å¯¹è¯åˆ›å»ºæ‘˜è¦"""
    if len(history) < 5:  # å¤ªçŸ­ä¸éœ€è¦æ‘˜è¦
        return None
    
    summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯åˆ›å»ºç®€æ´æ‘˜è¦ï¼š
    
å¯¹è¯å†…å®¹ï¼š
{chr(10).join([f'{type(msg).__name__}: {msg.content}' for msg in history[-10:]])}

è¦æ±‚ï¼š
1. ä¿ç•™å…³é”®ä¿¡æ¯å’Œä¸Šä¸‹æ–‡
2. æ§åˆ¶åœ¨100å­—ä»¥å†…
3. çªå‡ºå¯¹è¯ä¸»é¢˜å’Œè¿›å±•"""

    summary_model = init_model(model)
    summary_response = summary_model.invoke(summary_prompt)
    
    return f"[å¯¹è¯æ‘˜è¦: {summary_response.content}]"

def smart_history_management(model, history):
    """æ™ºèƒ½å†å²ç®¡ç†"""
    # å¦‚æœå†å²å¤ªé•¿ï¼Œåˆ›å»ºæ‘˜è¦
    if len(history) > 15:
        summary = create_conversation_summary(model, history[:-5])  # ä¸ºå‰é¢çš„å†…å®¹åˆ›å»ºæ‘˜è¦
        if summary:
            # ç”¨æ‘˜è¦æ›¿æ¢æ—©æœŸå¯¹è¯
            condensed_history = [history[0]]  # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            if summary:
                condensed_history.append(SystemMessage(summary))
            condensed_history.extend(history[-5:])  # ä¿ç•™æœ€è¿‘5æ¡æ¶ˆæ¯
            return condensed_history
    
    return history
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ

### 1. å¯¹è¯çŠ¶æ€æ¢å¤

```python
class ResilientConversation:
    def __init__(self, model):
        self.model = init_model(model)
        self.history = []
        self.checkpoint_file = "conversation_checkpoint.json"
    
    def save_checkpoint(self):
        """ä¿å­˜å¯¹è¯æ£€æŸ¥ç‚¹"""
        import json
        serializable_history = []
        
        for msg in self.history:
            msg_dict = {
                'type': type(msg).__name__,
                'content': msg.content
            }
            serializable_history.append(msg_dict)
        
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_history, f, ensure_ascii=False, indent=2)
    
    def load_checkpoint(self):
        """åŠ è½½å¯¹è¯æ£€æŸ¥ç‚¹"""
        import json
        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.history = []
            for msg_data in data:
                if msg_data['type'] == 'SystemMessage':
                    self.history.append(SystemMessage(msg_data['content']))
                elif msg_data['type'] == 'HumanMessage':
                    self.history.append(HumanMessage(msg_data['content']))
                elif msg_data['type'] == 'AIMessage':
                    self.history.append(AIMessage(msg_data['content']))
                    
            print("âœ… å¯¹è¯å†å²æ¢å¤æˆåŠŸ")
            return True
        except FileNotFoundError:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return False
        except Exception as e:
            print(f"âŒ æ¢å¤å¯¹è¯å†å²å¤±è´¥: {e}")
            return False
    
    def robust_chat(self, user_input):
        """å…·æœ‰å®¹é”™èƒ½åŠ›çš„å¯¹è¯"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_msg = HumanMessage(user_input)
            self.history.append(user_msg)
            
            # è°ƒç”¨æ¨¡å‹
            response = self.model.invoke(self.history)
            ai_msg = AIMessage(response.content)
            self.history.append(ai_msg)
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if len(self.history) % 5 == 0:
                self.save_checkpoint()
            
            return response.content
            
        except Exception as e:
            print(f"âŒ å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
            if self.load_checkpoint():
                return "ğŸ”„ ç³»ç»Ÿå·²æ¢å¤ï¼Œè¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜"
            else:
                return "âŒ ç³»ç»Ÿé‡åˆ°ä¸¥é‡é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•"
```

### 2. ä¸Šä¸‹æ–‡æº¢å‡ºå¤„ç†

```python
def handle_context_overflow(model, history, new_message):
    """å¤„ç†ä¸Šä¸‹æ–‡é•¿åº¦è¶…å‡ºé™åˆ¶çš„æƒ…å†µ"""
    MAX_CONTEXT_LENGTH = 3000  # æ ¹æ®æ¨¡å‹é™åˆ¶è°ƒæ•´
    
    # è®¡ç®—å½“å‰ä¸Šä¸‹æ–‡é•¿åº¦
    current_length = sum(len(getattr(msg, 'content', '').split()) 
                        for msg in history + [new_message])
    
    if current_length < MAX_CONTEXT_LENGTH:
        return history + [new_message]
    
    # å¤„ç†ä¸Šä¸‹æ–‡æº¢å‡º
    print("âš ï¸  ä¸Šä¸‹æ–‡å³å°†è¶…å‡ºé™åˆ¶ï¼Œæ­£åœ¨è¿›è¡Œä¼˜åŒ–...")
    
    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    optimized_history = [msg for msg in history if isinstance(msg, SystemMessage)]
    
    # æ·»åŠ æœ€æ–°çš„å‡ æ¡æ¶ˆæ¯
    recent_messages = [msg for msg in history if not isinstance(msg, SystemMessage)][-8:]
    optimized_history.extend(recent_messages)
    
    # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œè¿›ä¸€æ­¥å‹ç¼©
    while sum(len(getattr(msg, 'content', '').split()) 
              for msg in optimized_history + [new_message]) >= MAX_CONTEXT_LENGTH:
        if len(optimized_history) <= 2:  # è‡³å°‘ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            break
        optimized_history = optimized_history[1:]  # ç§»é™¤æœ€æ—©çš„æ¶ˆæ¯
    
    optimized_history.append(new_message)
    print(f"âœ… ä¸Šä¸‹æ–‡å·²ä¼˜åŒ–ï¼Œå½“å‰é•¿åº¦: {sum(len(getattr(msg, 'content', '').split()) for msg in optimized_history)} tokens")
    
    return optimized_history
```

## ğŸ“Š å¯¹è¯è´¨é‡è¯„ä¼°

### 1. è¿è´¯æ€§æ£€æµ‹

```python
def evaluate_conversation_coherence(model, history):
    """è¯„ä¼°å¯¹è¯è¿è´¯æ€§"""
    if len(history) < 4:  # è‡³å°‘éœ€è¦ä¸¤è½®å¯¹è¯
        return {"score": 0, "feedback": "å¯¹è¯è½®æ¬¡ä¸è¶³"}
    
    evaluation_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹å¯¹è¯çš„è¿è´¯æ€§ï¼š

å¯¹è¯å†…å®¹ï¼š
{chr(10).join([f'{i+1}. {type(msg).__name__}: {msg.content}' for i, msg in enumerate(history)])}

è¯„ä¼°æ ‡å‡†ï¼š
1. è¯é¢˜ä¸€è‡´æ€§ï¼ˆæ»¡åˆ†25åˆ†ï¼‰
2. é€»è¾‘è¿è´¯æ€§ï¼ˆæ»¡åˆ†25åˆ†ï¼‰  
3. å›åº”ç›¸å…³æ€§ï¼ˆæ»¡åˆ†25åˆ†ï¼‰
4. ä¸Šä¸‹æ–‡åˆ©ç”¨ï¼ˆæ»¡åˆ†25åˆ†ï¼‰

è¯·ç»™å‡ºæ€»åˆ†ï¼ˆ0-100ï¼‰å’Œç®€è¦è¯„è¯­ã€‚"""

    evaluator = init_model(model)
    evaluation = evaluator.invoke(evaluation_prompt)
    
    return {
        "score": extract_score(evaluation.content),
        "feedback": evaluation.content
    }

def extract_score(text):
    """ä»è¯„ä¼°æ–‡æœ¬ä¸­æå–åˆ†æ•°"""
    import re
    scores = re.findall(r'\d+', text)
    return int(scores[0]) if scores else 50  # é»˜è®¤50åˆ†
```

### 2. ç”¨æˆ·æ»¡æ„åº¦è¿½è¸ª

```python
class ConversationAnalytics:
    def __init__(self):
        self.metrics = {
            'total_turns': 0,
            'avg_response_length': 0,
            'user_interruptions': 0,
            'topic_changes': 0
        }
    
    def track_conversation(self, history):
        """è·Ÿè¸ªå¯¹è¯æŒ‡æ ‡"""
        self.metrics['total_turns'] = len([msg for msg in history if isinstance(msg, (HumanMessage, AIMessage))])
        
        ai_responses = [msg.content for msg in history if isinstance(msg, AIMessage)]
        if ai_responses:
            self.metrics['avg_response_length'] = sum(len(resp) for resp in ai_responses) / len(ai_responses)
    
    def generate_report(self):
        """ç”Ÿæˆå¯¹è¯åˆ†ææŠ¥å‘Š"""
        return f"""
ğŸ“Š å¯¹è¯åˆ†ææŠ¥å‘Š:
- æ€»å¯¹è¯è½®æ¬¡: {self.metrics['total_turns']}
- å¹³å‡å›å¤é•¿åº¦: {self.metrics['avg_response_length']:.1f} å­—ç¬¦
- ç”¨æˆ·æ‰“æ–­æ¬¡æ•°: {self.metrics['user_interruptions']}
- è¯é¢˜è½¬æ¢æ¬¡æ•°: {self.metrics['topic_changes']}
        """

# ä½¿ç”¨ç¤ºä¾‹
analytics = ConversationAnalytics()
# åœ¨å¯¹è¯è¿‡ç¨‹ä¸­è°ƒç”¨ analytics.track_conversation(history)
```

## ğŸ¨ é«˜çº§åº”ç”¨ç¤ºä¾‹

### 1. å¤šè§’è‰²å¯¹è¯ç³»ç»Ÿ

```python
class MultiCharacterDialogue:
    def __init__(self, model):
        self.model = init_model(model)
        self.characters = {}
        self.dialogue_history = []
    
    def add_character(self, name, personality):
        """æ·»åŠ å¯¹è¯è§’è‰²"""
        self.characters[name] = {
            'personality': personality,
            'speaking_history': []
        }
    
    def character_speak(self, speaker_name, message):
        """ç‰¹å®šè§’è‰²å‘è¨€"""
        if speaker_name not in self.characters:
            raise ValueError(f"è§’è‰² {speaker_name} ä¸å­˜åœ¨")
        
        # æ„é€ åŒ…å«æ‰€æœ‰è§’è‰²èƒŒæ™¯çš„æç¤º
        context_prompt = f"""åœºæ™¯è®¾å®šï¼š
{chr(10).join([f'{name}: {char["personality"]}' for name, char in self.characters.items()])}

å¯¹è¯å†å²ï¼š
{chr(10).join([f'{entry["speaker"]}: {entry["message"]}' for entry in self.dialogue_history[-10:]])}

ç°åœ¨è¯·ä»¥ {speaker_name} çš„èº«ä»½å›åº”ï¼š{message}
æ³¨æ„ä¿æŒè§’è‰²æ€§æ ¼ä¸€è‡´æ€§ã€‚"""

        response = self.model.invoke(context_prompt)
        
        # è®°å½•å¯¹è¯
        self.dialogue_history.append({
            'speaker': speaker_name,
            'message': message,
            'timestamp': time.time()
        })
        
        ai_response = response.content
        self.dialogue_history.append({
            'speaker': speaker_name,
            'message': ai_response,
            'timestamp': time.time()
        })
        
        return ai_response

# ä½¿ç”¨ç¤ºä¾‹
multi_char = MultiCharacterDialogue(model)
multi_char.add_character("æç™½", "è±ªæ”¾ä¸ç¾çš„æµªæ¼«ä¸»ä¹‰è¯—äºº")
multi_char.add_character("æœç”«", "å¿§å›½å¿§æ°‘çš„ç°å®ä¸»ä¹‰è¯—äºº")
print(multi_char.character_speak("æç™½", "äººç”Ÿå¾—æ„é¡»å°½æ¬¢"))
print(multi_char.character_speak("æœç”«", "å®‰å¾—å¹¿å¦åƒä¸‡é—´"))
```

### 2. æƒ…æ„ŸçŠ¶æ€å¯¹è¯ç³»ç»Ÿ

```python
class EmotionalConversation:
    def __init__(self, model):
        self.model = init_model(model)
        self.emotional_state = "neutral"
        self.mood_history = []
    
    def update_emotion(self, user_input, ai_response):
        """æ ¹æ®å¯¹è¯å†…å®¹æ›´æ–°æƒ…æ„ŸçŠ¶æ€"""
        emotion_prompt = f"""åˆ†æä»¥ä¸‹å¯¹è¯çš„æƒ…æ„Ÿèµ°å‘ï¼š

ç”¨æˆ·: {user_input}
AI: {ai_response}

å½“å‰æƒ…æ„ŸçŠ¶æ€: {self.emotional_state}

è¯·åˆ¤æ–­æƒ…æ„Ÿå˜åŒ–å¹¶ç»™å‡ºæ–°çš„æƒ…æ„ŸçŠ¶æ€ï¼ˆhappy/sad/angry/excited/calm/confused/neutralï¼‰ã€‚"""

        emotion_analyzer = init_model(model)
        emotion_response = emotion_analyzer.invoke(emotion_prompt)
        
        new_emotion = extract_emotion(emotion_response.content)
        if new_emotion != self.emotional_state:
            print(f"ğŸ˜Š æƒ…æ„ŸçŠ¶æ€å˜åŒ–: {self.emotional_state} â†’ {new_emotion}")
            self.emotional_state = new_emotion
        
        self.mood_history.append({
            'timestamp': time.time(),
            'emotion': self.emotional_state
        })
    
    def get_emotion_adjusted_response(self, prompt):
        """æ ¹æ®å½“å‰æƒ…æ„ŸçŠ¶æ€è°ƒæ•´å›å¤é£æ ¼"""
        emotional_prompt = f"""å½“å‰æƒ…æ„ŸçŠ¶æ€: {self.emotional_state}
è¯·æ ¹æ®è¿™ä¸ªæƒ…æ„ŸçŠ¶æ€è°ƒæ•´ä½ çš„å›å¤è¯­æ°”å’Œé£æ ¼ã€‚

åŸå§‹è¯·æ±‚: {prompt}

è¦æ±‚:
- happy: ç§¯æä¹è§‚ï¼Œå……æ»¡æ´»åŠ›
- sad: æ¸©æŸ”å…³æ€€ï¼Œå¯Œæœ‰åŒæƒ…å¿ƒ  
- angry: å†·é™ç†æ€§ï¼Œé¿å…æ¿€åŒ–çŸ›ç›¾
- excited: çƒ­æƒ…æ´‹æº¢ï¼Œå¯Œæœ‰æ„ŸæŸ“åŠ›
- calm: å¹³å’Œç†æ€§ï¼Œé€»è¾‘æ¸…æ™°
- confused: è€å¿ƒè§£é‡Šï¼Œå¾ªåºæ¸è¿›
- neutral: å®¢è§‚ä¸­ç«‹ï¼Œä¸“ä¸šä¸¥è°¨"""

        response = self.model.invoke(emotional_prompt)
        return response.content

# ä½¿ç”¨ç¤ºä¾‹
emotional_bot = EmotionalConversation(model)
response = emotional_bot.get_emotion_adjusted_response("ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½")
emotional_bot.update_emotion("ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½", response)
```

## ğŸ“ æ€»ç»“

å¤šè½®å¯¹è¯æ˜¯æ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ï¼š

âœ… **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šæœ‰æ•ˆç»´æŠ¤å¯¹è¯å†å²å’ŒçŠ¶æ€  
âœ… **è§’è‰²åŒºåˆ†**ï¼šæ­£ç¡®ä½¿ç”¨ä¸åŒç±»å‹çš„æ¶ˆæ¯  
âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šæ™ºèƒ½å‹ç¼©å’Œæ‘˜è¦æœºåˆ¶  
âœ… **å®¹é”™å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸æ¢å¤æœºåˆ¶  

## ğŸ”— ç›¸å…³èµ„æº

- [LangChain Memory Documentation](https://python.langchain.com/docs/modules/memory/)
- [Conversation Chain Guide](https://python.langchain.com/docs/modules/chains/popular/chat_models)
- [Message Types Reference](https://python.langchain.com/docs/modules/model_io/chat/messages)

---
*æœ¬æ•™ç¨‹æ·±å…¥è§£æäº†å¤šè½®å¯¹è¯çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä¸‹ä¸€æœŸæˆ‘ä»¬å°†æ¢ç´¢å·¥å…·è°ƒç”¨çš„å¼ºå¤§åŠŸèƒ½ã€‚*