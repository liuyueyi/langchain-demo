"""
æµå¼è°ƒç”¨çš„åœºæ™¯æ¼”ç¤º
"""
import os
from pathlib import Path

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model

config_path = os.path.join(Path(__file__).resolve().parents[2], 'config.env')
load_dotenv(config_path)

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = os.getenv('API_KEY')
os.environ["OPENAI_BASE_URL"] = os.getenv('BASE_URL')
model = os.getenv('MODEL')


def enhanced_stream_output(prompt, model):
    """å¢å¼ºç‰ˆæµå¼è¾“å‡ºï¼Œæ”¯æŒæ›´å¤šæ§åˆ¶é€‰é¡¹"""
    full_response = ""
    token_stats = {"input": 0, "output": 0}

    print("ğŸ¤– AIæ­£åœ¨æ€è€ƒä¸­...")
    print("-" * 50)

    try:
        for chunk in model.stream(prompt):
            # å®æ—¶è¾“å‡ºå†…å®¹
            if hasattr(chunk, 'content') and chunk.content:
                print(chunk.content, end='', flush=True)
                full_response += chunk.content

            # æ”¶é›†tokenç»Ÿè®¡
            if hasattr(chunk, 'usage_metadata'):
                usage = chunk.usage_metadata
                if usage:
                    token_stats["input"] = usage.get("input_tokens", 0)
                    token_stats["output"] = usage.get("output_tokens", 0)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†æµå¼è¾“å‡º")
    except Exception as e:
        print(f"\n\nâŒ æµå¼è°ƒç”¨å‡ºé”™: {e}")

    return full_response, token_stats


def init_model(model):
    # åˆå§‹åŒ– LLM Model
    return init_chat_model(model=model,
                           model_provider="openai",  # æŒ‡å®šæ¨¡å‹å‚å•†
                           temperature=0.7,  # æ¸©åº¦ï¼Œæ§åˆ¶è¿”å›æ›´ç¨³å®šè¿˜æ˜¯æ›´æœ‰åˆ›é€ åŠ›çš„ç»“æœ
                           timeout=30,  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’
                           max_tokens=1000,  # é™åˆ¶å“åº”ä¸­çš„ä»¤ç‰Œæ€»æ•°ï¼Œä»è€Œæœ‰æ•ˆåœ°æ§åˆ¶è¾“å‡ºçš„é•¿åº¦ã€‚
                           max_retries=3,  # æœ€å¤§å¤±è´¥é‡è¯•æ¬¡æ•°
                           )


def long_text_streaming(model_name):
    """é•¿æ–‡æœ¬æµå¼ç”Ÿæˆç¤ºä¾‹"""

    model = init_model(model_name)

    long_prompt = """è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„è¯¦ç»†æ–‡ç« ï¼Œ
    åŒ…æ‹¬æŠ€æœ¯è¶‹åŠ¿ã€ç¤¾ä¼šå½±å“ã€ä¼¦ç†è€ƒé‡ç­‰æ–¹é¢ï¼Œè‡³å°‘1000å­—ã€‚"""

    print("ğŸ“ å¼€å§‹ç”Ÿæˆé•¿ç¯‡æ–‡ç« ...")
    full_text, stats = enhanced_stream_output(long_prompt, model)

    print(f"\n\nğŸ“‹ æ–‡ç« ç»Ÿè®¡:")
    print(f"å­—æ•°: {len(full_text)} å­—ç¬¦")
    print(f"Tokenæ¶ˆè€—: è¾“å…¥{stats['input']}, è¾“å‡º{stats['output']}")
    print(f"ç”Ÿæˆæ•ˆç‡: {stats['output'] / len(full_text):.2f} tokens/å­—ç¬¦")


long_text_streaming(model)


def code_generation_stream(model_name):
    """ä»£ç ç”Ÿæˆçš„æµå¼é¢„è§ˆ"""
    code_prompt = "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ï¼Œå¹¶æ·»åŠ è¯¦ç»†æ³¨é‡Š"

    print("ğŸ’» æ­£åœ¨ç”Ÿæˆä»£ç ...")
    print("=" * 60)

    model = init_model(model_name)
    for chunk in model.stream(code_prompt):
        # ä»£ç é«˜äº®æ•ˆæœæ¨¡æ‹Ÿ
        if chunk.content:
            if 'def' in chunk.content or 'class' in chunk.content:
                print(f"\033[94m{chunk.content}\033[0m", end='')  # è“è‰²
            elif 'import' in chunk.content:
                print(f"\033[92m{chunk.content}\033[0m", end='')  # ç»¿è‰²
            elif '#' in chunk.content:
                print(f"\033[93m{chunk.content}\033[0m", end='')  # é»„è‰²
            else:
                print(chunk.content, end='')

    print("\n" + "=" * 60)
    print("âœ… ä»£ç ç”Ÿæˆå®Œæˆ!")
