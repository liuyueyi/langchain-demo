import os

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.env')
load_dotenv(config_path)

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = os.getenv('API_KEY')
os.environ["OPENAI_BASE_URL"] = os.getenv('BASE_URL')
model = os.getenv('MODEL')


def pretty_print_ai_response_prefix(response_type="sync"):
    separator = "=" * 60

    print(f"\n{separator}")
    if response_type == "stream":
        print("ğŸ¤– AI æµå¼å›å¤ä¸­...")
    else:
        print("ğŸ¤– AI æ™ºèƒ½å›å¤")
    print(separator)


def pretty_print_ai_response_suffix(response):
    separator = "=" * 60
    # æŠ€æœ¯ä¿¡æ¯
    print(f"\n{separator}")
    print("ğŸ“Š æŠ€æœ¯è¯¦æƒ…:")
    print(f"  ğŸ“ ç±»å‹: {type(response).__name__}")

    # Token ä½¿ç”¨æƒ…å†µ
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        print(f"  ğŸ’° Token: {response.usage_metadata}")
    elif hasattr(response, 'usage') and response.usage:
        print(f"  ğŸ’° Token: {response.usage}")
    else:
        print("  ğŸ’° Token: æœªæä¾›")

    # å¯¹è±¡å±æ€§ç»Ÿè®¡
    attr_count = len([attr for attr in dir(response) if not attr.startswith('_')])
    print(f"  ğŸ” å±æ€§æ•°: {attr_count} ä¸ª")
    print(separator)


def pretty_print_ai_response(response):
    """
    ç¾åŒ–çš„ AI å“åº”è¾“å‡º
    :param response: å¤§æ¨¡å‹çš„è¿”å›
    :return:
    """
    pretty_print_ai_response_prefix("sync")

    # ä¸»è¦å†…å®¹æ˜¾ç¤º
    print(f"\nğŸ’¬ å›å¤å†…å®¹:")
    if hasattr(response, 'content'):
        print(response.content)
    else:
        print(str(response))

    pretty_print_ai_response_suffix(response)


def init_model(model):
    # åˆå§‹åŒ– LLM Model
    return init_chat_model(model=model,
                           model_provider="openai",  # æŒ‡å®šæ¨¡å‹å‚å•†
                           temperature=0.7,  # æ¸©åº¦ï¼Œæ§åˆ¶è¿”å›æ›´ç¨³å®šè¿˜æ˜¯æ›´æœ‰åˆ›é€ åŠ›çš„ç»“æœ
                           timeout=30,  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’
                           max_tokens=1000,  # é™åˆ¶å“åº”ä¸­çš„ä»¤ç‰Œæ€»æ•°ï¼Œä»è€Œæœ‰æ•ˆåœ°æ§åˆ¶è¾“å‡ºçš„é•¿åº¦ã€‚
                           max_retries=3,  # æœ€å¤§å¤±è´¥é‡è¯•æ¬¡æ•°
                           )


def multi_turn_invoke(model):
    """
    å¤šè½®å¯¹è¯
    :param model:
    :return:
    """
    # åˆå§‹åŒ– LLM Model
    model = init_model(model)

    conversation = [
        # ç³»ç»Ÿæç¤ºè¯
        {"role": "system", "content": "ä½ ç°åœ¨æ‰®æ¼”ç››å”æœ€è‘—åçš„å¤§è¯—äººæç™½ï¼Œä»¥ç‹‚æ”¾ä¸ç¾ã€é£˜é€¸æ¢¦å¹»ã€å¤§æ°”ç£…ç¤´çš„é£æ ¼è‘—ç§°"},
        # ç”¨æˆ·çš„é—®ç­”
        {"role": "user", "content": "è¯·å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜æœˆå…‰çš„å¤è¯—"},
        # æ¨¡å‹å›ç­”
        {"role": "assistant", "content": """ã€Šæ˜æœˆå…‰èµ‹ã€‹
é’å¤©è£‚é•œè½ä¹ç§‹ï¼Œå†°é­„åˆæ‚¬æ»¡ç¥å·ã€‚
æ¬²å€Ÿé“¶æ²³æ–ŸåŒ—æ–—ï¼Œé†‰å€¾ç‰å£¶ç™½ç‰ç§‹ã€‚
æ¸…è¾‰æ¼«æ´’å¦‚ç§‹éœœåˆƒï¼Œç¢å½±å¾˜å¾Šä¼¼å¤œçœ¸ã€‚
é†‰èˆå¹¿å¯’å®«é˜™å¤–ï¼Œæ‰¶æ‘‡ç›´ä¸Šç ´è‹ç©¹ã€‚
æ˜æœˆç…§æˆ‘æ„æœªå°½ï¼Œä¸”é‚€æ¸…è¾‰é†‰å¿ƒç”°ã€‚"""},
        {"role": "user", "content": "æˆ‘å¸Œæœ›åœ¨ä¸Šé¢çš„è¿”å›ä¸­ï¼Œæ·»åŠ ä¸€äº›å…³äºä»™äººã€ä¾ å®¢çš„å†…å®¹"},
    ]
    # æ·»åŠ ç³»ç»Ÿæç¤º
    response = model.invoke(conversation)
    pretty_print_ai_response(response)


print("--" * 30 + " modelä¼ è¾“å¤šè½®å¯¹è¯(ç³»ç»Ÿ+ç”¨æˆ·æç¤º) " + "--" * 30)
multi_turn_invoke(model)


def multi_turn_invoke_v2(model):
    """
    å¤šè½®å¯¹è¯
    :param model:
    :return:
    """
    # åˆå§‹åŒ– LLM Model
    model = init_model(model)
    # ä¸ä¸Šé¢çš„åŒºåˆ«åœ¨äºå‰é¢ä¼ jsonä¼ ï¼Œè¿™é‡Œæ˜¯é€šè¿‡ message ç±» æ¥åŒºåˆ†æ¶ˆæ¯ç±»å‹ï¼Œé˜…è¯»æ›´å‹å¥½
    conversation = [
        # ç³»ç»Ÿæç¤ºè¯
        SystemMessage("ä½ ç°åœ¨æ‰®æ¼”ç››å”æœ€è‘—åçš„å¤§è¯—äººæç™½ï¼Œä»¥ç‹‚æ”¾ä¸ç¾ã€é£˜é€¸æ¢¦å¹»ã€å¤§æ°”ç£…ç¤´çš„é£æ ¼è‘—ç§°"),
        # ç”¨æˆ·çš„é—®ç­”
        HumanMessage("è¯·å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜æœˆå…‰çš„å¤è¯—"),
        # æ¨¡å‹å›ç­”
        AIMessage("""ã€Šæ˜æœˆå…‰èµ‹ã€‹
é’å¤©è£‚é•œè½ä¹ç§‹ï¼Œå†°é­„åˆæ‚¬æ»¡ç¥å·ã€‚
æ¬²å€Ÿé“¶æ²³æ–ŸåŒ—æ–—ï¼Œé†‰å€¾ç‰å£¶ç™½ç‰ç§‹ã€‚
æ¸…è¾‰æ¼«æ´’å¦‚ç§‹éœœåˆƒï¼Œç¢å½±å¾˜å¾Šä¼¼å¤œçœ¸ã€‚
é†‰èˆå¹¿å¯’å®«é˜™å¤–ï¼Œæ‰¶æ‘‡ç›´ä¸Šç ´è‹ç©¹ã€‚
æ˜æœˆç…§æˆ‘æ„æœªå°½ï¼Œä¸”é‚€æ¸…è¾‰é†‰å¿ƒç”°ã€‚"""),
        HumanMessage("æˆ‘å¸Œæœ›åœ¨ä¸Šé¢çš„è¿”å›ä¸­ï¼Œæ·»åŠ ä¸€äº›å…³äºä»™äººã€ä¾ å®¢çš„å†…å®¹"),
    ]
    # æ·»åŠ ç³»ç»Ÿæç¤º
    response = model.invoke(conversation)
    pretty_print_ai_response(response)
