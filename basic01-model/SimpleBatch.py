import os

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model

config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.env')
load_dotenv(config_path)

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = os.getenv('API_KEY')
os.environ["OPENAI_BASE_URL"] = os.getenv('BASE_URL')
model = os.getenv('MODEL')


def pretty_print_ai_response_prefix(response_type="sync"):
    separator = "=" * 60

    print(f"\n{separator}")
    if response_type == "stream":
        print("ğŸ¤– AI æµå¼å›å¤ä¸­...")
    else:
        print("ğŸ¤– AI æ™ºèƒ½å›å¤")
    print(separator)


def pretty_print_ai_response_suffix(response):
    separator = "=" * 60
    # æŠ€æœ¯ä¿¡æ¯
    print(f"\n{separator}")
    print("ğŸ“Š æŠ€æœ¯è¯¦æƒ…:")
    print(f"  ğŸ“ ç±»å‹: {type(response).__name__}")

    # Token ä½¿ç”¨æƒ…å†µ
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        print(f"  ğŸ’° Token: {response.usage_metadata}")
    elif hasattr(response, 'usage') and response.usage:
        print(f"  ğŸ’° Token: {response.usage}")
    else:
        print(f"  ğŸ’° Token: æœªæä¾›")

    # å¯¹è±¡å±æ€§ç»Ÿè®¡
    attr_count = len([attr for attr in dir(response) if not attr.startswith('_')])
    print(f"  ğŸ” å±æ€§æ•°: {attr_count} ä¸ª")
    print(separator)


def init_model(model):
    # åˆå§‹åŒ– LLM Model
    return init_chat_model(model=model,
                           model_provider="openai",  # æŒ‡å®šæ¨¡å‹å‚å•†
                           temperature=0.7,  # æ¸©åº¦ï¼Œæ§åˆ¶è¿”å›æ›´ç¨³å®šè¿˜æ˜¯æ›´æœ‰åˆ›é€ åŠ›çš„ç»“æœ
                           timeout=30,  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’
                           max_tokens=1000,  # é™åˆ¶å“åº”ä¸­çš„ä»¤ç‰Œæ€»æ•°ï¼Œä»è€Œæœ‰æ•ˆåœ°æ§åˆ¶è¾“å‡ºçš„é•¿åº¦ã€‚
                           max_retries=3,  # æœ€å¤§å¤±è´¥é‡è¯•æ¬¡æ•°
                           )


def batch_call(model):
    # æ‰¹é‡è°ƒç”¨
    model = init_model(model)

    # # æµå¼è°ƒç”¨ï¼Œè¿”å›æ•´ä¸ªæ‰¹æ¬¡çš„æœ€ç»ˆè¾“å‡º
    # responses = model.batch([
    #     "å†™ä¸€é¦–å…³äºæœˆå…‰çš„äº”è¨€ç»å¥",
    #     "å†™ä¸€é¦–å…³äºç§‹å¤©çš„ä¸ƒè¨€å¾‹è¯—",
    #     "å†™ä¸€é¦–å…³äºçª—å°çš„ç°ä»£è¯—"
    # ])

    pretty_print_ai_response_prefix("sync")

    # ä¸»è¦å†…å®¹æ˜¾ç¤º
    print(f"\nğŸ’¬ å›å¤å†…å®¹:")
    for res in model.batch_as_completed([
        "å†™ä¸€é¦–å…³äºæœˆå…‰çš„äº”è¨€ç»å¥",
        "å†™ä¸€é¦–å…³äºç§‹å¤©çš„ä¸ƒè¨€å¾‹è¯—",
        "å†™ä¸€é¦–å…³äºçª—å°çš„ç°ä»£è¯—"
    ]):
        # æ¯ä¸ªè¾“å…¥ç”Ÿæˆå®Œæˆä¹‹åç«‹å³æ¥æ”¶è¿”å›
        index, response = res
        if hasattr(response, 'content'):
            print(f"åºå· {index}: {response.content}")
        else:
            print(f"åºå· {index}: {response}")

        pretty_print_ai_response_suffix(response)


print("--" * 30 + " æ‰¹é‡è°ƒç”¨ " + "--" * 30)
batch_call(model)
